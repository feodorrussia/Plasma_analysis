{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T13:09:46.601598Z",
     "start_time": "2024-09-09T13:09:43.991844Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from source.Files_operating import read_sht_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "import random\n",
    "\n",
    "from matplotlib.colors import CSS4_COLORS as COLORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T14:21:11.463899Z",
     "start_time": "2024-09-09T14:21:11.448103Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def get_boarders(data: np.array, loc_max_ind=None, scale=1.5):\n",
    "    loc_max_ind = np.argmax(data)\n",
    "    dist_ind = np.argsort(np.abs(data - data[loc_max_ind] / scale))\n",
    "    return Slice(dist_ind[dist_ind <= loc_max_ind][0], dist_ind[dist_ind >= loc_max_ind][0])\n",
    "\n",
    "\n",
    "class Slice:\n",
    "    def __init__(self, start_index=0, end_index=0):\n",
    "        self.l = start_index\n",
    "        self.r = end_index\n",
    "        self.mark = 1.0\n",
    "\n",
    "    def set_boarders(self, start_index: int, end_index: int) -> None:\n",
    "        self.l = start_index\n",
    "        self.r = end_index\n",
    "\n",
    "    def set_mark(self, mark: int) -> None:\n",
    "        self.mark = mark\n",
    "\n",
    "    def copy(self):\n",
    "        new_slice = Slice(self.l, self.r)\n",
    "        new_slice.set_mark(new_slice.mark)\n",
    "        return new_slice\n",
    "\n",
    "    def check_length(self, len_edge: int) -> bool:\n",
    "        return self.r - self.l > len_edge\n",
    "\n",
    "    def check_dist(self, other, dist_edge: int) -> bool:\n",
    "        return other.l - self.r > dist_edge\n",
    "\n",
    "    def collide_slices(self, other, dist_edge: int) -> bool:\n",
    "        if not self.check_dist(other, dist_edge):\n",
    "            self.r = other.r\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def step(self) -> None:\n",
    "        self.r += 1\n",
    "\n",
    "    def move(self, delta: int) -> None:\n",
    "        self.r += delta\n",
    "        self.l += delta\n",
    "\n",
    "    def expand(self, delta: int) -> None:\n",
    "        self.r += delta\n",
    "        self.l -= delta\n",
    "\n",
    "    def collapse_boarders(self) -> None:\n",
    "        self.l = self.r\n",
    "\n",
    "    def is_null(self) -> bool:\n",
    "        return self.l == self.r\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"({self.l}, {self.r})\"\n",
    "\n",
    "\n",
    "class Signal_meta:\n",
    "    def __init__(self, chanel_name=\"da\", processing_flag=False, \n",
    "                 quantile_edge=0.0, std_edge=1.0, \n",
    "                 length_edge=10, distance_edge=10, scale=1.5, step_out=10, \n",
    "                 std_bottom_edge=0, std_top_edge=1, d_std_bottom_edge=3, d_std_top_edge=6, amplitude_ratio=0.5):\n",
    "        self.name = chanel_name\n",
    "        self.proc_fl = processing_flag\n",
    "        \n",
    "        self.len_edge = length_edge\n",
    "        self.dist_edge = distance_edge\n",
    "        self.scale = scale\n",
    "        self.step_out = step_out\n",
    "        \n",
    "        self.q = quantile_edge\n",
    "        self.std = std_edge\n",
    "        \n",
    "        self.d_q = quantile_edge\n",
    "        self.d_std = std_edge\n",
    "\n",
    "        self.std_top = std_top_edge\n",
    "        self.std_bottom = std_bottom_edge\n",
    "        self.d_std_top = d_std_top_edge\n",
    "        self.d_std_bottom = d_std_bottom_edge\n",
    "\n",
    "        self.max_min_ratio = amplitude_ratio\n",
    "        \n",
    "    def set_statistics(self, data: np.array, data_diff: np.array, percentile: float, d_percentile: float, std_bottom_edge=0, std_top_edge=1, d_std_bottom_edge=3, d_std_top_edge=6, amplitude_ratio=0.5):\n",
    "        self.q = np.quantile(data, percentile)\n",
    "        self.std = data.std()\n",
    "        self.std_top = std_top_edge\n",
    "        self.std_bottom = std_bottom_edge\n",
    "        \n",
    "        self.d_q = np.quantile(data_diff, percentile)\n",
    "        self.d_std = data_diff.std()\n",
    "        if self.name == \"sxr\":\n",
    "            a = 10.5\n",
    "            b = -850\n",
    "            self.d_std_top = d_std_top_edge / d_std_bottom_edge * a * np.exp(b * self.d_std)\n",
    "            self.d_std_bottom = a * np.exp(b * self.d_std)\n",
    "        else:\n",
    "            self.d_std_top = d_std_top_edge\n",
    "            self.d_std_bottom = d_std_bottom_edge            \n",
    "\n",
    "        self.max_min_ratio = amplitude_ratio\n",
    "\n",
    "    def set_edges(self, length_edge=10, distance_edge=10, scale=1.5, step_out=10):\n",
    "        self.len_edge = length_edge\n",
    "        self.dist_edge = distance_edge\n",
    "        self.scale = scale\n",
    "        self.step_out = step_out\n",
    "\n",
    "\n",
    "def get_peaks(data: np.array, s_i: int) -> np.array:\n",
    "    peaks_ind = []\n",
    "    loc_max = data.min()\n",
    "    m_v = data.mean()\n",
    "    loc_max_ind = 0\n",
    "    increase_fl = False\n",
    "    for i in range(data.shape[0] - 1):\n",
    "        if loc_max < data[i]:\n",
    "            increase_fl = True\n",
    "            loc_max = data[i]\n",
    "            loc_max_ind = i\n",
    "        elif abs((loc_max - data[i]) / (loc_max + 1e-10)) > 0.5 and increase_fl:\n",
    "            if abs(loc_max) > 5 * abs(m_v + 1e-10):\n",
    "                peaks_ind.append(loc_max_ind)\n",
    "            # print(s_i + loc_max_ind, abs((loc_max - data[i]) / loc_max), abs((loc_max - m_v) / m_v))  # len(peaks_ind),\n",
    "            increase_fl = False\n",
    "\n",
    "        if not increase_fl or data[i] < data[i + 1]:\n",
    "            loc_max = data[i]\n",
    "            loc_max_ind = i\n",
    "    return np.array(peaks_ind)\n",
    "\n",
    "def get_boarders_d2(data:np.array, diff_data: np.array, s_i: int, scale=1.5):\n",
    "    d2_data = np.diff(diff_data)\n",
    "    peaks_ind = get_peaks(d2_data, s_i)\n",
    "\n",
    "    if len(peaks_ind) == 0:\n",
    "        return Slice(0, diff_data.shape[0])\n",
    "    if len(peaks_ind) == 1:\n",
    "        scale_slice = get_boarders(data, scale=scale)\n",
    "        # print(scale_slice)\n",
    "        if peaks_ind[0] < diff_data.shape[0] - peaks_ind[0]:\n",
    "            if peaks_ind[0] - scale_slice.r < 0:\n",
    "                return Slice(peaks_ind[0], scale_slice.r)\n",
    "            else:\n",
    "                return Slice(peaks_ind[0], diff_data.shape[0])\n",
    "        else:\n",
    "            if peaks_ind[0] - scale_slice.l > 0:\n",
    "                return Slice(scale_slice.l, peaks_ind[0])\n",
    "            else:\n",
    "                return Slice(0, peaks_ind[0])\n",
    "    return Slice(peaks_ind[0], peaks_ind[-1])\n",
    "\n",
    "def proc_boarders(data: np.array, data_diff: np.array, start_ind: int, scale=1.5) -> Slice:\n",
    "    step = 5\n",
    "    \n",
    "    diff_coeff = 1\n",
    "    if data_diff[start_ind] < 0:\n",
    "        diff_coeff = -1\n",
    "    \n",
    "    cur_ind = (start_ind + diff_coeff * step) if 0 < start_ind + diff_coeff * step < data.shape[0] else start_ind\n",
    "    while 0 < cur_ind + diff_coeff * step < data.shape[0] and data_diff[cur_ind] * data_diff[cur_ind + diff_coeff * step] > 0:\n",
    "        cur_ind += diff_coeff * step\n",
    "\n",
    "    # print(cur_ind - 3 * step, cur_ind + 3 * step, end=\" \")\n",
    "    max_ind = np.argmax(data[max(cur_ind - 3 * step, 0): min(cur_ind + 3 * step, data.shape[0])]) + cur_ind - step\n",
    "    length = max(abs(max_ind - start_ind), 3 * step)\n",
    "    # print(max_ind, length)\n",
    "\n",
    "    # print(max_ind - length, max_ind + 2 * length, end=\" \")\n",
    "    res_slice = get_boarders_d2(data[max(max_ind - length, 0): min(max_ind + 2 * length, data.shape[0])], data_diff[max(max_ind - length, 0): min(max_ind + 2 * length, data.shape[0])], max(max_ind - length, 0), scale=scale)  # get_boarders(data[max_ind - length: max_ind + 2 * length], loc_max_ind=length) | Slice(0, 3 * length)\n",
    "    res_slice.move(max_ind - length)\n",
    "    # print(res_slice.l, res_slice.r)\n",
    "\n",
    "    # add checking diff on right & left boarder (cut on D2 peaks) - done\n",
    "    # add dtw classification (None | ELM | LSO)\n",
    "\n",
    "    # print(res_slice.l, res_slice.r, end=\" \")\n",
    "    # print(abs(data_diff[res_slice.l:res_slice.r].max() - np.quantile(data_diff, 0.7)), data_diff.std())\n",
    "    \n",
    "    return res_slice\n",
    "\n",
    "\n",
    "def proc_slices(mark_data: np.array, data: np.array, data_diff: np.array, meta: Signal_meta) -> np.array:  # data: np.array, , scale=1.5 , step_out=10\n",
    "    proc_slice = Slice(0, 50)\n",
    "    cur_slice = Slice(0, 51)\n",
    "    f_fragment = False\n",
    "\n",
    "    res_mark = np.copy(mark_data)\n",
    "    res_mark[cur_slice.l: cur_slice.r] = 0.0\n",
    "    cur_slice.collapse_boarders()\n",
    "    proc_slice = cur_slice.copy()\n",
    "\n",
    "    c = 0\n",
    "    \n",
    "    while cur_slice.r < res_mark.shape[0]:\n",
    "        if res_mark[cur_slice.r] == 1.0:\n",
    "            if not f_fragment:\n",
    "                f_fragment = True\n",
    "        elif f_fragment:\n",
    "            # print(start_ind, end_ind)\n",
    "            if not cur_slice.check_length(meta.len_edge):\n",
    "                res_mark[cur_slice.l: cur_slice.r] = 0.0\n",
    "            elif not proc_slice.collide_slices(cur_slice, meta.dist_edge):\n",
    "                if meta.proc_fl and meta.scale > 1:\n",
    "                    res_mark[proc_slice.l: proc_slice.r] = 0.0\n",
    "                    start_ind = proc_slice.l if data_diff[proc_slice.l] > 0 else proc_slice.r\n",
    "                    proc_slice = proc_boarders(data, data_diff, start_ind, meta.scale)\n",
    "                    \n",
    "                    cur_slice = Slice(proc_slice.r, proc_slice.r)\n",
    "                \n",
    "                proc_slice.expand(meta.step_out)\n",
    "\n",
    "                if meta.proc_fl and abs(data_diff[proc_slice.l:proc_slice.r].max() - meta.d_q) < meta.d_std_top * meta.d_std and \\\n",
    "                   abs(data_diff[proc_slice.l:proc_slice.r].min() - meta.d_q) < meta.d_std_top * meta.d_std:\n",
    "                    proc_slice.set_mark(0)\n",
    "                \n",
    "                if meta.name == \"sxr\":\n",
    "                    if abs(abs(data_diff[proc_slice.l:proc_slice.r].min()) - meta.d_q) > meta.d_std_top * meta.d_std:\n",
    "                        proc_slice.set_mark(1)\n",
    "                        \n",
    "                    if abs(data_diff[proc_slice.l:proc_slice.r].max() / data_diff[proc_slice.l:proc_slice.r].min()) < meta.max_min_ratio:\n",
    "                        proc_slice.set_mark(1)\n",
    "                    else:\n",
    "                        proc_slice.set_mark(0)\n",
    "                \n",
    "                res_mark[proc_slice.l: proc_slice.r] = proc_slice.mark\n",
    "                c += proc_slice.mark\n",
    "                \n",
    "                proc_slice = cur_slice.copy()\n",
    "            f_fragment = False\n",
    "            cur_slice.collapse_boarders()\n",
    "        elif not f_fragment:\n",
    "            cur_slice.collapse_boarders()\n",
    "            if proc_slice.is_null():\n",
    "                proc_slice = cur_slice.copy()\n",
    "    \n",
    "        cur_slice.step()\n",
    "    # print(c)\n",
    "\n",
    "    return res_mark\n",
    "\n",
    "\n",
    "def get_slices(mark_data: np.array):\n",
    "    cur_slice = Slice(0, 1)\n",
    "    f_fragment = False\n",
    "\n",
    "    slices_list = []\n",
    "    \n",
    "    while cur_slice.r < mark_data.shape[0]:\n",
    "        if mark_data[cur_slice.r] == 1.0:\n",
    "            if not f_fragment:\n",
    "                f_fragment = True\n",
    "        elif f_fragment:\n",
    "            slices_list.append(copy.copy(cur_slice))\n",
    "            f_fragment = False\n",
    "            cur_slice.collapse_boarders()\n",
    "        elif not f_fragment:\n",
    "            cur_slice.collapse_boarders()\n",
    "        cur_slice.step()\n",
    "\n",
    "    return slices_list\n",
    "\n",
    "\n",
    "def get_d2_peaks(diff_data: np.array, mark_data: np.array, s_i: int):\n",
    "    l_ = 0\n",
    "    fr_fl = False\n",
    "    peaks_ind = []\n",
    "    for i in range(mark_data.shape[0]):\n",
    "        if mark_data[i] == 1 and not fr_fl:\n",
    "            fr_fl = True\n",
    "            l_ = i\n",
    "        elif fr_fl and mark_data[i] == 0:\n",
    "            fr_fl = False\n",
    "            d2_data = np.diff(diff_data)\n",
    "            res = get_peaks(d2_data[l_-10:i+30], s_i) + l_-10\n",
    "            peaks_ind += res.tolist()\n",
    "    peaks_ind = np.array(peaks_ind)\n",
    "    return peaks_ind, d2_data[peaks_ind]\n",
    "\n",
    "\n",
    "def zero_bin_search(arr: np.array):\n",
    "    l = 0\n",
    "    r = arr.shape[0] - 1\n",
    "    if arr[l] * arr[r] > 0 or r - l <= 0:\n",
    "        return -1\n",
    "    \n",
    "    while r - l > 1:\n",
    "        m = (r - l) // 2 + l\n",
    "        if arr[l] * arr[m] <= 0:\n",
    "            r = m\n",
    "        else:\n",
    "            l = m\n",
    "    \n",
    "    return r if arr[l] > arr[r] else l\n",
    "\n",
    "\n",
    "def get_d1_crosses(d1_data: np.array, d2_data: np.array, start: int, end: int, d1_coef=1) -> np.array:\n",
    "    d1_std = d1_data.std()\n",
    "    d1_m = d1_data.mean()\n",
    "    d2_std = d2_data.std()\n",
    "    d2_m = d2_data.mean()\n",
    "    \n",
    "    cur_slice = Slice(start, start + 1)\n",
    "    f_slice = False\n",
    "    ans = []\n",
    "    while cur_slice.r < end + 1:\n",
    "        if abs(d1_data[cur_slice.r] - d1_m) < d1_std * d1_coef and d2_data[cur_slice.r] - d2_m < d2_std / 3 * 2:\n",
    "            if not f_slice and (d1_data[cur_slice.r - 1] - d1_m) > d1_std * d1_coef:\n",
    "                f_slice = True\n",
    "        \n",
    "        elif f_slice:  #  and d1_data[cur_slice.r] < 0\n",
    "            # print(cur_slice)\n",
    "            if d1_data[cur_slice.r] > d1_m:\n",
    "                cur_slice.r = np.argmin(d1_data[cur_slice.l: cur_slice.r]) + cur_slice.l\n",
    "                \n",
    "            if d1_data[cur_slice.l] >= d1_m and d1_data[cur_slice.r] <= d1_m:\n",
    "                # print(cur_slice)\n",
    "                zero_i = zero_bin_search(d1_data[cur_slice.l: cur_slice.r])\n",
    "                if zero_i != -1:\n",
    "                    ans.append(zero_i + cur_slice.l)\n",
    "            \n",
    "            f_slice = False\n",
    "            cur_slice.collapse_boarders()\n",
    "            \n",
    "        elif not f_slice:\n",
    "            cur_slice.collapse_boarders()\n",
    "    \n",
    "        cur_slice.step()\n",
    "\n",
    "    if f_slice:\n",
    "        if d1_data[cur_slice.r] > d1_m:\n",
    "            cur_slice.r = np.argmin(d1_data[cur_slice.l: cur_slice.r]) + cur_slice.l\n",
    "            \n",
    "        if d1_data[cur_slice.l] >= d1_m and d1_data[cur_slice.r] <= d1_m:\n",
    "            zero_i = zero_bin_search(d1_data[cur_slice.l: cur_slice.r])\n",
    "            if zero_i != -1:\n",
    "                ans.append(zero_i + cur_slice.l)\n",
    "    return np.array(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T13:10:12.772791Z",
     "start_time": "2024-09-09T13:10:07.640437Z"
    }
   },
   "outputs": [],
   "source": [
    "F_ID = 44375\n",
    "proj_path = \"C:/Users/f.belous/Work/Projects/Plasma_analysis\"\n",
    "# D:/Edu/Lab/Projects/Plasma_analysis | C:/Users/f.belous/Work/Projects/Plasma_analysis\n",
    "dir_path = proj_path + \"/data/sht/NG-ELM/\"  \n",
    "\n",
    "df = read_sht_data(f'sht{F_ID}', dir_path)\n",
    "df = df.rename(columns={\"ch1\": \"d_alpha\"})\n",
    "df[\"sxr\"] = read_sht_data(f'sht{F_ID}', dir_path, data_name=\"SXR 50 mkm\").ch1\n",
    "# dbs = read_dataFile(f'data/dbs/{F_ID} DBS.dat')\n",
    "# mgd_data_tor\n",
    "# mgd_data_vertical\n",
    "df[\"mgd_v\"] = read_sht_data(f'sht{F_ID}', dir_path, data_name=\"МГД быстрый зонд верт.\").ch1\n",
    "# mgd_data_radial\n",
    "df[\"mgd_r\"] = read_sht_data(f'sht{F_ID}', dir_path, data_name=\"МГД быстрый зонд рад.\").ch1\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T13:10:17.649171Z",
     "start_time": "2024-09-09T13:10:17.628135Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "d_alpha = df.d_alpha.to_numpy()\n",
    "sxr = df.sxr.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T13:10:18.100388Z",
     "start_time": "2024-09-09T13:10:18.068168Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "d_alpha_d1 = np.diff(d_alpha)\n",
    "sxr_d1 = np.diff(sxr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T13:10:26.340302Z",
     "start_time": "2024-09-09T13:10:22.341690Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "\n",
    "b, a = signal.butter(5, 0.1)\n",
    "d_alpha_f = signal.filtfilt(b, a, d_alpha_d1)\n",
    "b, a = signal.butter(5, 0.05)\n",
    "sxr_f = signal.filtfilt(b, a, sxr_d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_groups(groups):\n",
    "    groups.sort(key=lambda x: len(x), reverse=True)\n",
    "    d_groups = {}\n",
    "    for i in range(len(groups)):\n",
    "        d_groups[i] = True\n",
    "    res_groups = []\n",
    "    i = 0\n",
    "    while i < len(groups):\n",
    "        j = 0\n",
    "        while j < len(groups):  # for every group we check all next groups about subsequention\n",
    "            if i == j:\n",
    "                j += 1\n",
    "                continue\n",
    "            arr1 = groups[i]\n",
    "            arr2 = groups[j]\n",
    "            if arr1[0] >= arr2[0]:  # check that arrays are consistent\n",
    "                j += 1\n",
    "                continue\n",
    "            k = check_subsequention(arr1, arr2)  # get len of subsequention\n",
    "            if k != -1 and k != len(arr2):  # check valide & not full subsequention\n",
    "                d_groups[i] = False\n",
    "                d_groups[j] = False\n",
    "                res_arr = arr1 + arr2[k:]\n",
    "                if not array_equasion(groups[-1], res_arr):\n",
    "                    groups.append(res_arr)\n",
    "                    res_groups.append(res_arr)\n",
    "            j += 1\n",
    "        i += 1\n",
    "\n",
    "    for i in range(len(groups)):\n",
    "        if d_groups[i]:\n",
    "            res_groups.append(groups[i])\n",
    "    return res_groups\n",
    "\n",
    "\n",
    "def check_subsequention(arr1, arr2) -> int:    \n",
    "    n = min(len(arr1), len(arr2))\n",
    "    # print(arr1, arr2, n)\n",
    "    for i in range(1, n + 1):\n",
    "        # print(i, arr1[-i:], arr2[:i])\n",
    "        if arr1[-i] < arr2[0]:\n",
    "            return -1\n",
    "        if array_equasion(arr1[-i:], arr2[:i]):\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "\n",
    "def array_equasion(arr1, arr2) -> bool:\n",
    "    if len(arr1) != len(arr2):\n",
    "        return False\n",
    "    for i in range(len(arr1)):\n",
    "        if arr1[i] != arr2[i]:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def in_array(arr1, arr2) -> bool:\n",
    "    if len(arr1) > len(arr2):\n",
    "        return False\n",
    "    for i in range(len(arr2) - len(arr1) + 1):\n",
    "        if array_equasion(arr1, arr2[i:i + len(arr1)]):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def get_valide_groups_from_struct_by_delta(groups_struct, delta, DELTA_DELTA, MIN_GROUP, DELTA_MAX):  # groups_struct = {Point: {Delta: [mistake: float, group: list]}}\n",
    "    res_groups = []\n",
    "    if delta < DELTA_MAX:\n",
    "        # get groups from delta\n",
    "        for item in groups_struct[delta]:\n",
    "            if len(item[1]) > MIN_GROUP:\n",
    "                res_groups.append(item[1])\n",
    "                \n",
    "        # get groups from delta +- delta_delta \n",
    "        for d2 in groups_struct.keys():\n",
    "            if d != d2 and d2 < DELTA_MAX and abs(d - d2) / d <= DELTA_DELTA:\n",
    "                for item in groups_struct[d2]:\n",
    "                    if len(item[1]) > MIN_GROUP:\n",
    "                        res_groups.append(item[1])\n",
    "    return res_groups\n",
    "\n",
    "\n",
    "def get_valide_groups_from_struct_by_amplitude(groups_struct, d_alpha, peaks):\n",
    "    res_groups = []\n",
    "    g_set = set()\n",
    "    for delta in groups_struct.keys():\n",
    "        for item in groups_struct[delta]:\n",
    "            p_arr = item[1]\n",
    "            n = len(p_arr)\n",
    "            cur_group = [p_arr[0]]\n",
    "            group_m_amplitude = d_alpha[peaks[cur_group]][0]\n",
    "            \n",
    "            if n <= 2 and p_arr[1] != p_arr[0] + 1:  # check valid by consistance on 0 elemnt\n",
    "                continue\n",
    "            \n",
    "            for i in range(1, n):\n",
    "                if i < n - 1 and p_arr[i + 1] != p_arr[i] + 1:  # check valid by consistance on i elemnt\n",
    "                    if abs(d_alpha[peaks[p_arr[i]]] - group_m_amplitude) / max(group_m_amplitude, d_alpha[peaks[p_arr[i]]]) <= 0.34:\n",
    "                        cur_group.append(p_arr[i])\n",
    "                        group_m_amplitude = np.nanmean(d_alpha[peaks[cur_group]])\n",
    "                    break\n",
    "                \n",
    "                if abs(d_alpha[peaks[p_arr[i]]] - group_m_amplitude) / max(group_m_amplitude, d_alpha[peaks[p_arr[i]]]) <= 0.34:\n",
    "                    cur_group.append(p_arr[i])\n",
    "                    group_m_amplitude = np.nanmean(d_alpha[peaks[cur_group]])\n",
    "                elif d_alpha[peaks[p_arr[i]]] > group_m_amplitude:\n",
    "                    if len(cur_group) > 1:\n",
    "                        res_groups.append(cur_group)\n",
    "                    \n",
    "                    cur_group = [p_arr[i]]\n",
    "                    group_m_amplitude = d_alpha[peaks[cur_group]][0]\n",
    "            if len(cur_group) > 0:\n",
    "                group_str = \"/\".join(list(map(str, cur_group)))\n",
    "                if group_str not in g_set:\n",
    "                    g_set.add(group_str)\n",
    "                    res_groups.append(cur_group)\n",
    "                \n",
    "    return res_groups\n",
    "\n",
    "\n",
    "def get_unique_point_from_groups(groups):\n",
    "    set_p = set()\n",
    "    for group in groups:\n",
    "        set_p = set_p.union(set(group))\n",
    "    return sorted(list(set_p))\n",
    "\n",
    "\n",
    "def get_groups_2(arr): # -> groups = {Delta: [mistake: float, group: list]}\n",
    "    n = len(arr)\n",
    "    res_struct = {}\n",
    "    if n < 2:\n",
    "        return res_struct\n",
    "    cur_delta = arr[1] - arr[0]\n",
    "    cur_group = [0, 1]\n",
    "    cur_err = 0.0\n",
    "    for i in range(2, n):\n",
    "        err = abs(arr[i] - (arr[cur_group[-1]] + cur_delta)) - int(random.random() * 10)\n",
    "        if cur_err + err < cur_delta:  # check valide\n",
    "            cur_err += err\n",
    "            cur_group.append(i)\n",
    "        else:  # save cur group & upd pointer\n",
    "            if cur_delta in res_struct.keys():\n",
    "                res_struct[cur_delta].append([cur_err, cur_group])\n",
    "                res_struct[cur_delta].sort(key=lambda x: len(x[1]), reverse=True)\n",
    "            else:\n",
    "                res_struct[cur_delta] = [[cur_err, cur_group]]\n",
    "            \n",
    "            cur_delta = arr[i] - arr[cur_group[-1]]\n",
    "            cur_group = [cur_group[-1], i]\n",
    "            cur_err = 0.0\n",
    "    \n",
    "    if cur_delta in res_struct.keys():\n",
    "        res_struct[cur_delta].append([cur_err, cur_group])\n",
    "        res_struct[cur_delta].sort(key=lambda x: len(x[1]), reverse=True)\n",
    "    else:\n",
    "        res_struct[cur_delta] = [[cur_err, cur_group]]\n",
    "    return res_struct\n",
    "\n",
    "\n",
    "def get_groups(arr): # -> groups = {Delta: [mistake: float, group: list]}\n",
    "    n = len(arr)\n",
    "    res_struct = {}\n",
    "    if n < 2:\n",
    "        return res_struct\n",
    "    cur_group = []\n",
    "\n",
    "    stac_p =  list(range(n - 2, -1, -1))\n",
    "    while len(stac_p) > 0:\n",
    "        # print(stac_p, cur_group)\n",
    "        ind = stac_p.pop()\n",
    "        # print(ind)\n",
    "        fl_save = False\n",
    "        if len(cur_group) == 0:\n",
    "            cur_delta = arr[ind + 1] - arr[ind]\n",
    "            cur_group = [ind, ind + 1]\n",
    "            cur_err = 0.0\n",
    "\n",
    "            if ind < n - 2:\n",
    "                stac_p.append(ind + 2)\n",
    "            else:\n",
    "                fl_save = True\n",
    "        else:\n",
    "            while ind < n - 2 and abs(arr[ind] - (arr[cur_group[-1]] + cur_delta)) >= abs(arr[ind + 1] - (arr[cur_group[-1]] + cur_delta)):\n",
    "                if ind not in stac_p:\n",
    "                    stac_p.append(ind)\n",
    "                ind += 1\n",
    "\n",
    "            err = abs(arr[ind] - (arr[cur_group[-1]] + cur_delta))\n",
    "            if cur_err + err < cur_delta:  # check valide\n",
    "                cur_err += err\n",
    "                cur_group.append(ind)\n",
    "                if ind < n - 1:\n",
    "                    stac_p.append(ind + 1)\n",
    "                else:\n",
    "                    fl_save = True\n",
    "            else:  # save cur group & upd pointer\n",
    "                fl_save = True\n",
    "                stac_p.append(ind - 1)\n",
    "    \n",
    "        if fl_save or ind == n - 1:\n",
    "            if cur_delta in res_struct.keys():\n",
    "                res_struct[cur_delta].append([cur_err, cur_group])\n",
    "                res_struct[cur_delta].sort(key=lambda x: len(x[1]), reverse=True)\n",
    "            else:\n",
    "                res_struct[cur_delta] = [[cur_err, cur_group]]\n",
    "            cur_group = []\n",
    "        \n",
    "    return res_struct\n",
    "\n",
    "\n",
    "def get_time_delta(arr):\n",
    "    return (arr[-1] - arr[0]) / (len(arr) - 1)\n",
    "\n",
    "\n",
    "def merge_peaks(points, d_alpha):\n",
    "    d = get_time_delta(points)\n",
    "    res = [points[0]]\n",
    "    for i in range(1, len(points)):\n",
    "        if (points[i] - res[-1]) / d < 0.7:\n",
    "            res[-1] = points[i] if d_alpha[points[i]] > d_alpha[points[i - 1]] else points[i - 1]\n",
    "        else:\n",
    "            res.append(points[i])\n",
    "    return res\n",
    "\n",
    "def get_groups_from_signal(d_alpha, d_alpha_f, d_alpha_d2f, l_edge, r_edge):  # -> groups = [group: list(time points)]\n",
    "    DELTA_DELTA = 0.1 # \n",
    "    DELTA_MAX = 1000 # points\n",
    "    MIN_GROUP = 2 # min num points in group\n",
    "\n",
    "    # get peaks on diagnostic (in the one group)\n",
    "    peaks = np.array(get_d1_crosses(d_alpha_f, d_alpha_d2f, l_edge, r_edge))\n",
    "    # m_, std_ = np.mean(d_alpha[pre_peaks]), np.std(d_alpha[pre_peaks])\n",
    "    # peaks_ind = np.argwhere((m_ - d_alpha[pre_peaks]) / std_ < 1).transpose()[0]  # ((m_ - d_alpha[pre_peaks]) / std_ < 1) | ((m_ - d_alpha[pre_peaks]) / std_ > 10)\n",
    "    # peaks = pre_peaks[peaks_ind]\n",
    "    # print(peaks_ind)\n",
    "    # print((m_ - d_alpha[peaks]) / std_)\n",
    "\n",
    "    # check amplitude\n",
    "\n",
    "    # # divide peaks to groups\n",
    "    prev_peaks = []\n",
    "    cur_peaks = copy.copy(peaks)\n",
    "    while not array_equasion(prev_peaks, cur_peaks):\n",
    "        # print(\"- logg: \", cur_peaks)\n",
    "        # save peaks time points\n",
    "        prev_peaks = copy.copy(cur_peaks)\n",
    "        # get struct of groups = {Delta: [mistake: float, group: list]}\n",
    "        all_groups_struct = get_groups(cur_peaks)\n",
    "        # print(\"- logg: \", all_groups_struct)\n",
    "        # get list of valid groups by amplitude\n",
    "        valid_groups = get_valide_groups_from_struct_by_amplitude(all_groups_struct, d_alpha, cur_peaks)\n",
    "        valid_groups = list(map(lambda x: merge_peaks(x, d_alpha) if len(x) > 1 else x, valid_groups))\n",
    "        # print(\"- logg: \", valid_groups)\n",
    "        # get list of unique peaks indeces\n",
    "        peaks_ind = get_unique_point_from_groups(valid_groups)\n",
    "        # get new peaks time points\n",
    "        cur_peaks = copy.copy(cur_peaks[peaks_ind])\n",
    "    \n",
    "    all_groups_struct = get_groups(cur_peaks)\n",
    "    # print(\"- logg: \", all_groups_struct)\n",
    "    valid_groups = get_valide_groups_from_struct_by_amplitude(all_groups_struct, d_alpha, cur_peaks)\n",
    "    # print(\"- logg: \", sorted(valid_groups, key=lambda x: x[0]))\n",
    "    \n",
    "    # # group post processing: union by delta & get missing\n",
    "    # res_groups = []\n",
    "    # for d in sorted(all_groups_struct.keys()):\n",
    "    #     # get all groups from d & d+-delta_delta\n",
    "    #     delta_groups = get_valide_groups_from_struct_by_delta(all_groups_struct, d, DELTA_DELTA, MIN_GROUP, DELTA_MAX)\n",
    "    #     # combine subsequent groups\n",
    "    #     res_groups += combine_groups(delta_groups)\n",
    "    valid_groups = list(map(lambda x: merge_peaks(x, d_alpha) if len(x) > 1 else x, valid_groups))\n",
    "    valid_groups = list(filter(lambda x: len(x) > 1, sorted(valid_groups, key=lambda x: x[0])))\n",
    "    res_peaks = [cur_peaks[valid_groups[0]]]\n",
    "    for gr_indeces in valid_groups[1:]:\n",
    "        d1, d2 = get_time_delta(cur_peaks[gr_indeces]), get_time_delta(res_peaks[-1])\n",
    "        # print(\"- logg: \", res_peaks[-1], gr_indeces, abs(d1 - d2) / d1)\n",
    "        if (0.3 < abs(d1 - d2) / d1 < 0.8 and d1 > d2) or (0.3 < abs(d1 - d2) / d1 < 1.5 and d1 <= d2):\n",
    "            # print(\"- logg: \", res_peaks[-1], gr_indeces, abs(d1 - d2) / d1)\n",
    "            # res_peaks.append(merge_peaks(cur_peaks[gr_indeces], d_alpha))\n",
    "            res_peaks.append(cur_peaks[gr_indeces])\n",
    "            continue\n",
    "        \n",
    "        k = check_subsequention(res_peaks[-1], cur_peaks[gr_indeces])  # get len of subsequention\n",
    "        if k != -1 and k != len(cur_peaks[gr_indeces]):  # check valide & not full subsequention\n",
    "            res_arr = np.concatenate([res_peaks[-1], cur_peaks[gr_indeces][k:]])\n",
    "            # print(\"- logg: \", res_peaks[-1], cur_peaks[gr_indeces], res_arr)\n",
    "            # res_peaks[-1] = copy.copy(merge_peaks(res_arr, d_alpha))\n",
    "            res_peaks[-1] = copy.copy(merge_peaks(res_arr, d_alpha))\n",
    "        elif in_array(res_peaks[-1], cur_peaks[gr_indeces]):\n",
    "            # print(\"- logg: \", res_peaks[-1], cur_peaks[gr_indeces])\n",
    "            # res_peaks[-1] = copy.copy(merge_peaks(cur_peaks[gr_indeces], d_alpha))\n",
    "            res_peaks[-1] = copy.copy(cur_peaks[gr_indeces])\n",
    "        elif k != len(cur_peaks[gr_indeces]) and not in_array(cur_peaks[gr_indeces], res_peaks[-1]):\n",
    "            # print(\"- logg: \", k, gr_indeces)\n",
    "            # res_peaks.append(merge_peaks(cur_peaks[gr_indeces], d_alpha))\n",
    "            res_peaks.append(cur_peaks[gr_indeces])\n",
    "    return res_peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import AutoMinorLocator\n",
    "\n",
    "\n",
    "slices_df = pd.read_csv(proj_path + f\"/data/df/marks/{F_ID}_marks.csv\")\n",
    "slices_edges = slices_df.to_numpy()[:, :2]\n",
    "slices_marks = slices_df.to_numpy()[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['gold', 'brown', 'black', 'seagreen', 'skyblue', 'cyan', 'yellow', 'violet', 'royalblue', 'sandybrown', 'grey', 'gray', 'indigo', 'rosybrown', 'darkviolet', 'coral', 'pink', 'magenta', 'red', 'springgreen', 'darkblue', 'silver', 'seashell', 'green', 'navy', 'purple', 'sienna', 'chocolate', 'orange', 'blue']\n",
    "ind = 4  # 3, 4, 7\n",
    "l_edge, r_edge = slices_edges[ind]\n",
    "\n",
    "fig, ax1 = plt.subplots(nrows=1, sharex=True, gridspec_kw={'hspace': 0.2})\n",
    "\n",
    "fig.set_figwidth(16)\n",
    "fig.set_figheight(4)\n",
    "\n",
    "ax1.set_title(f\"#{F_ID}\")\n",
    "\n",
    "b, a = signal.butter(5, 0.1)\n",
    "d_alpha_d2f = signal.filtfilt(b, a, np.diff(d_alpha_f))\n",
    "\n",
    "ax1.plot(range(l_edge, r_edge), d_alpha[l_edge:r_edge], label=\"D-alpha\", alpha=0.8, zorder=2)\n",
    "# ax1.plot(range(l_edge, r_edge), d_alpha_d1[l_edge:r_edge], label=\"Diff 1\")\n",
    "ax1.plot(range(l_edge, r_edge), d_alpha_f[l_edge:r_edge] * 10, label=\"Filtered D1 (x10)\", alpha=0.8)\n",
    "ax1.plot(range(l_edge, r_edge), d_alpha_d2f[l_edge:r_edge] * 100, label=\"D2 (x100)\", alpha=0.8)\n",
    "\n",
    "coef=1.\n",
    "x = get_d1_crosses(d_alpha_f, d_alpha_d2f, l_edge, r_edge, d1_coef=coef)\n",
    "print(f\"{ind + 1}/{slices_edges.shape[0]} - Slice ({l_edge/1e3}, {r_edge/1e3}) ms - {len(x)} peaks - {(time.time() - start_time)*1e3:.3f} ms\")\n",
    "# ax1.scatter(x, d_alpha[x], s=20, color=\"black\")\n",
    "\n",
    "print(f\"Start prossecing peaks ...\", end=\" \")  # \n",
    "start_time = time.time()\n",
    "res_groups_peaks = get_groups_from_signal(d_alpha, d_alpha_f, d_alpha_d2f, l_edge, r_edge)\n",
    "# print(\"- logg: \", res_groups_peaks)\n",
    "print(f\"- Tooks: {(time.time() - start_time)*1e3:.3f} ms\")\n",
    "for g_i in range(len(res_groups_peaks)):\n",
    "    points = res_groups_peaks[g_i]\n",
    "    c = colors[g_i]\n",
    "    ax1.scatter(points, d_alpha[points] + g_i * 0.1, s=20, color=c, zorder=0)\n",
    "    m_d = get_time_delta(points) / 1e3\n",
    "    print(f\"{g_i + 1}/{len(res_groups_peaks)} Group of peaks ({c}) - {len(points)} peaks in group - mean delta: {m_d:.3f} ms - freq: {1/m_d:.3f} kHz\")\n",
    "\n",
    "for i in range(len(x)):\n",
    "    num = i\n",
    "    d = 0\n",
    "    while num > 10:\n",
    "        num = num // 10\n",
    "        d += 1\n",
    "    ax1.annotate(i, (x[i] - 25 - 15 * d, d_alpha[x[i]] + 0.03))\n",
    "\n",
    "ax1.axhline(d_alpha_f.mean(), color=\"red\", linestyle=':', linewidth=0.8)\n",
    "ax1.axhline((d_alpha_f.mean() + d_alpha_f.std()) * 10 * coef, color=\"green\", linestyle=':', linewidth=0.8)\n",
    "ax1.axhline((d_alpha_f.mean() - d_alpha_f.std()) * 10 * coef, color=\"green\", linestyle=':', linewidth=0.8)\n",
    "ax1.axhline((d_alpha_d2f.mean() + d_alpha_d2f.std()) * 100, color=\"blue\", linestyle=':', linewidth=0.8)\n",
    "ax1.axhline((d_alpha_d2f.mean() - d_alpha_d2f.std()) * 100, color=\"blue\", linestyle=':', linewidth=0.8)\n",
    "ax1.grid(which='major', color='#DDDDDD', linewidth=0.9)\n",
    "ax1.grid(which='minor', color='#DDDDDD', linestyle=':', linewidth=0.7)\n",
    "ax1.minorticks_on()\n",
    "ax1.xaxis.set_minor_locator(AutoMinorLocator(10))\n",
    "ax1.legend(loc='lower right')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# print(\"----\")\n",
    "# for j in range(len(slices_edges)):\n",
    "#     print(f\"{j + 1}/{slices_edges.shape[0]}\")\n",
    "#     l_edge, r_edge = slices_edges[j]\n",
    "#     start_time = time.time()\n",
    "#     res_groups = get_groups_from_signal(d_alpha, d_alpha_f, d_alpha_d2f, l_edge, r_edge)\n",
    "#     print(f\"Took: {(time.time() - start_time)*1e3:.3f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gr_delts = []\n",
    "for d in sorted(res_groups.keys()):  # , key=lambda k: res_groups[k][0]\n",
    "    delts = [d]\n",
    "    for d2 in res_groups.keys():\n",
    "        if abs(d - d2) / d <= 0.1 and d != d2 and len(res_groups[d2][0][1]) > 2 and d2 < 1000:\n",
    "            delts.append(d2)\n",
    "    if sorted(delts) not in gr_delts:\n",
    "        gr_delts.append(sorted(delts))\n",
    "        if len(res_groups[d][0][1]) > 2 and d < 1000:\n",
    "            for x in sorted(delts, key=lambda k: len(res_groups[k][0][1]), reverse=True):\n",
    "                print(f\"{x}: {res_groups[x]}\")\n",
    "            print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in sorted(res_groups.keys(), key=lambda k: res_groups[k][0]):\n",
    "    if res_groups[d][1][:2] == [0, 3]:\n",
    "        print(f\"{d}: {res_groups[d]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_arr = []  # l_edge, r_edge, n, fr, fr_std\n",
    "\n",
    "for ind in range(slices_edges.shape[0]):\n",
    "    l_edge, r_edge = slices_edges[ind]\n",
    "\n",
    "    fig, ax1 = plt.subplots(nrows=1, sharex=True, gridspec_kw={'hspace': 0.2})\n",
    "    \n",
    "    fig.set_figwidth(16)\n",
    "    fig.set_figheight(4)\n",
    "    \n",
    "    ax1.set_title(f\"#{F_ID}\")\n",
    "    \n",
    "    b, a = signal.butter(5, 0.1)\n",
    "    d_alpha_d2f = signal.filtfilt(b, a, np.diff(d_alpha_f))\n",
    "    \n",
    "    ax1.plot(range(l_edge, r_edge), d_alpha[l_edge:r_edge], label=\"D-alpha\", alpha=0.8, zorder=2)\n",
    "    # ax1.plot(range(l_edge, r_edge), d_alpha_d1[l_edge:r_edge], label=\"Diff 1\")\n",
    "    ax1.plot(range(l_edge, r_edge), d_alpha_f[l_edge:r_edge] * 10, label=\"Filtered D1 (x10)\", alpha=0.8)\n",
    "    ax1.plot(range(l_edge, r_edge), d_alpha_d2f[l_edge:r_edge] * 100, label=\"D2 (x100)\", alpha=0.8)\n",
    "    \n",
    "    coef=1.\n",
    "    x = get_d1_crosses(d_alpha_f, d_alpha_d2f, l_edge, r_edge, d1_coef=coef)\n",
    "    print(f\"{ind + 1}/{slices_edges.shape[0]} - Slice ({l_edge/1e3}, {r_edge/1e3}) ms - mark: {slices_marks[ind]} (1 - dELM, 2 - LCO, 3 - EHO) - {len(x)} peaks - {(time.time() - start_time)*1e3:.3f} ms\")\n",
    "    # ax1.scatter(x, d_alpha[x], s=20, color=\"black\")\n",
    "    \n",
    "    print(f\"Start prossecing peaks ...\", end=\" \")  # \n",
    "    start_time = time.time()\n",
    "    res_groups_peaks = get_groups_from_signal(d_alpha, d_alpha_f, d_alpha_d2f, l_edge, r_edge)\n",
    "    # print(\"- logg: \", res_groups_peaks)\n",
    "    print(f\"- Tooks: {(time.time() - start_time)*1e3:.3f} ms\")\n",
    "    for g_i in range(len(res_groups_peaks)):\n",
    "        points = res_groups_peaks[g_i]\n",
    "        c = colors[g_i]\n",
    "        ax1.scatter(points, d_alpha[points] + g_i * 0.05, s=20, color=c, zorder=0)\n",
    "        \n",
    "        m_d = get_time_delta(points) / 1e3\n",
    "        std_d = 0\n",
    "        for p_i in range(1, len(points)):\n",
    "            std_d += (m_d - (points[p_i] - points[p_i - 1])) ** 2\n",
    "        std_d = (std_d / len(points) / (len(points) - 1)) ** .5\n",
    "        print(f\"{g_i + 1}/{len(res_groups_peaks)} Group of peaks ({c}) - {len(points)} peaks in group - mean delta: {m_d:.3f} ms - freq: {1/m_d:.3f} +- {std_d/(m_d ** 2):.3f} kHz\")\n",
    "    \n",
    "    for p_i in range(len(x)):\n",
    "        num = p_i\n",
    "        d = 0\n",
    "        while num > 10:\n",
    "            num = num // 10\n",
    "            d += 1\n",
    "        ax1.annotate(p_i, (x[p_i] - (25) * (r_edge - l_edge) / 5000 - 15 * d, d_alpha[x[p_i]] + 0.03))\n",
    "    \n",
    "    ax1.axhline(d_alpha_f.mean(), color=\"red\", linestyle=':', linewidth=0.8)\n",
    "    ax1.axhline((d_alpha_f.mean() + d_alpha_f.std()) * 10 * coef, color=\"green\", linestyle=':', linewidth=0.8)\n",
    "    ax1.axhline((d_alpha_f.mean() - d_alpha_f.std()) * 10 * coef, color=\"green\", linestyle=':', linewidth=0.8)\n",
    "    ax1.axhline((d_alpha_d2f.mean() + d_alpha_d2f.std()) * 100, color=\"blue\", linestyle=':', linewidth=0.8)\n",
    "    ax1.axhline((d_alpha_d2f.mean() - d_alpha_d2f.std()) * 100, color=\"blue\", linestyle=':', linewidth=0.8)\n",
    "    ax1.grid(which='major', color='#DDDDDD', linewidth=0.9)\n",
    "    ax1.grid(which='minor', color='#DDDDDD', linestyle=':', linewidth=0.7)\n",
    "    ax1.minorticks_on()\n",
    "    ax1.xaxis.set_minor_locator(AutoMinorLocator(10))\n",
    "    ax1.legend(loc='lower right')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    mode = int(input(\"Input mode [manual (input points) - 0 | auto (input group) - 1 | continue - -1]: \"))\n",
    "    while mode >= 0:\n",
    "        if mode == 0:\n",
    "            points_ind = list(map(int, input(\"Input point indexes:\\n\").strip().split()))\n",
    "            points = x[points_ind]\n",
    "        else:\n",
    "            gr_ind = int(input(f\"Input group number (from 1 to {len(res_groups_peaks)}): \").strip().split()[0]) - 1\n",
    "            points = res_groups_peaks[gr_ind]\n",
    "        \n",
    "        m_d = get_time_delta(points) / 1e3\n",
    "        std_d = 0\n",
    "        for p_i in range(1, len(points)):\n",
    "            std_d += (m_d - (points[p_i] - points[p_i - 1]) / 1e3) ** 2\n",
    "        std_d = (std_d / len(points) / (len(points) - 1)) ** .5\n",
    "        df_arr.append([points[0] - 50, points[-1] + 50, len(points), 1/m_d, std_d/(m_d ** 2), slices_marks[ind]])\n",
    "        # cur_deltas_desync_elm.std() / (cur_deltas_desync_elm.mean() ** 2)\n",
    "        mode = int(input(\"Input mode [manual (input points) - 0 | auto (input group) - 1 | continue - -1]: \"))\n",
    "        \n",
    "# l_edge, r_edge, n, fr, fr std\n",
    "df = pd.DataFrame(df_arr, columns=[\"l_edge\", \"r_edge\", \"n\", \"fr\", \"fr_std\", \"mark\"])\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f\"data/df/stats/{F_ID}_slices_stats.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo for combine groups\n",
    "groups = [[0, 1, 2, 3, 4, 5, 6], [4, 5, 6, 8, 11], [8, 11, 14]]\n",
    "\n",
    "start_time = time.time()\n",
    "k = check_subsequention(groups[0], groups[1])\n",
    "print(f\"Took: {(time.time() - start_time)*1e6:.3e} mcs\")\n",
    "print(f\"Arrays: {groups[0]}, {groups[1]} - k = {k} - Res array: {groups[0] + groups[1][k:]}\")\n",
    "\n",
    "print(\"----\")\n",
    "\n",
    "start_time = time.time()\n",
    "res_grops = combine_groups(groups)\n",
    "print(f\"Took: {(time.time() - start_time)*1e6:.3f} mcs\")\n",
    "print(res_grops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non phisical\n",
    "def get_groups_naive(arr): # -> groups = {Delta: [mistake: float, group: list]}\n",
    "    n = len(arr)\n",
    "    res = {}\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            delta = arr[j] - arr[i]\n",
    "            group = [i, j]\n",
    "            err = 0.0\n",
    "            for k in range(j + 2, n):\n",
    "                if abs(arr[k] - (arr[group[-1]] + delta)) >= abs(arr[k - 1] - (arr[group[-1]] + delta)):\n",
    "                    cur_err = abs(arr[k - 1] - (arr[group[-1]] + delta)) - int(random.random() * 10)\n",
    "                    if err + cur_err > delta / 3 * 2:  #  / len(group)\n",
    "                        break\n",
    "                    err += cur_err\n",
    "                    group.append(k - 1)\n",
    "            if delta in res.keys():\n",
    "                res[delta].append([err, group])\n",
    "                res[delta].sort(key=lambda x: len(x[1]), reverse=True)\n",
    "            else:\n",
    "                res[delta] = [[err, group]]\n",
    "    return res\n",
    "\n",
    "# def get_groups_dp(): # -> groups = {Delta: [mistake: float, group: list]}\n",
    "# def get_groups_gen(): # -> groups = {Delta: [mistake: float, group: list]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mgd = df.mgd_v.to_numpy() ** 2 + df.mgd_r.to_numpy() ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_da = Signal_meta(chanel_name=\"da\", processing_flag=True)\n",
    "meta_da.set_statistics(d_alpha, d_alpha_f, 0.7, 0.7, d_std_bottom_edge=1.5, d_std_top_edge=2.7)\n",
    "meta_da.set_edges(length_edge=15, distance_edge=30)\n",
    "\n",
    "meta_da_2 = Signal_meta(chanel_name=\"da\", processing_flag=False)\n",
    "meta_da_2.set_statistics(d_alpha, d_alpha_f, 0.7, 0.7, d_std_bottom_edge=1.5, d_std_top_edge=2.5)\n",
    "meta_da_2.set_edges(length_edge=15, distance_edge=30)\n",
    "\n",
    "meta_sxr = Signal_meta(chanel_name=\"sxr\", processing_flag=True)\n",
    "meta_sxr.set_statistics(sxr, sxr_f, 0.8, 0.8, d_std_bottom_edge=7., d_std_top_edge=15.0)\n",
    "meta_sxr.set_edges(length_edge=10, distance_edge=30, scale=0, step_out=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T14:59:43.298141Z",
     "start_time": "2024-09-09T14:59:42.537753Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mark_data = np.zeros(d_alpha_f.shape)\n",
    "mark_data[abs(d_alpha_f - meta_da.d_q) > meta_da.d_std * meta_da.d_std_bottom] = 1\n",
    "mark_d_alpha = proc_slices(mark_data, d_alpha, d_alpha_f, meta_da)\n",
    "\n",
    "mark_data = np.zeros(d_alpha_f.shape)\n",
    "mark_data[abs(d_alpha_f - meta_da_2.d_q) > meta_da_2.d_std * meta_da_2.d_std_bottom] = 1\n",
    "mark_d_alpha_2 = proc_slices(mark_data, d_alpha, d_alpha_f, meta_da_2)\n",
    "\n",
    "mark_data = np.zeros(d_alpha_f.shape)\n",
    "mark_data[abs(sxr_f - meta_sxr.d_q) > meta_sxr.d_std * meta_sxr.d_std_bottom] = 1\n",
    "mark_sxr = proc_slices(mark_data, sxr, sxr_f, meta_sxr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T14:59:44.820246Z",
     "start_time": "2024-09-09T14:59:43.934085Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import AutoMinorLocator\n",
    "\n",
    "\n",
    "mode_plotting = 0  # int(input(\"Input visualising mode [ 0 - manual | 1 - only marked ]:\"))\n",
    "# mode_marking = int(input(\"Input marking mode (0 - manual | 1 - semiauto): \").strip().split()[0])\n",
    "l_edge = 184000  # int(input(f\"Input start index [0:{df.shape[0]//1000*1000}]: \").strip().split()[0])\n",
    "# print(\"---\")\n",
    "step = 2000\n",
    "width = 5000\n",
    "\n",
    "step_away = 10\n",
    "\n",
    "while l_edge < df.shape[0]:\n",
    "    r_edge = min(l_edge + width, df.shape[0] - 1)\n",
    "    \n",
    "    if mode_plotting == 1 and sum(mark_d_alpha[l_edge:r_edge]) == 0 and sum(mark_sxr[l_edge:r_edge]) == 0:\n",
    "        l_edge += step\n",
    "        continue\n",
    "    \n",
    "    fig, [ax1, ax2, ax3] = plt.subplots(nrows=3, sharex=True, gridspec_kw={'hspace': 0.2})\n",
    "\n",
    "    fig.set_figwidth(16)\n",
    "    fig.set_figheight(12)\n",
    "    \n",
    "    ax1.set_title(f\"#{F_ID}\")\n",
    "\n",
    "    b, a = signal.butter(5, 0.1)\n",
    "    d_alpha_d2f = signal.filtfilt(b, a, np.diff(d_alpha_f))\n",
    "\n",
    "    ax1.plot(range(l_edge, r_edge), d_alpha[l_edge:r_edge], label=\"D-alpha\", alpha=0.8)\n",
    "    # ax1.plot(range(l_edge, r_edge), d_alpha_d1[l_edge:r_edge], label=\"Diff 1\")\n",
    "    ax1.plot(range(l_edge, r_edge), d_alpha_f[l_edge:r_edge] * 10, label=\"Filtered D1 (x10)\", alpha=0.8)\n",
    "    ax1.plot(range(l_edge, r_edge), d_alpha_d2f[l_edge:r_edge] * 100, label=\"D2 (x100)\", alpha=0.8)\n",
    "\n",
    "    x = get_d1_crosses(d_alpha_f, d_alpha_d2f, l_edge, r_edge)\n",
    "    print(get_groups_from_signal(d_alpha_f, d_alpha_d2f, l_edge, r_edge))\n",
    "    ax1.scatter(x, d_alpha[x], s=20, color=\"black\")\n",
    "\n",
    "    # ax1.plot(range(l_edge, r_edge), mark_d_alpha_2[l_edge:r_edge] * 0.4, label=\"Mark (x0.4)\", color=\"red\")\n",
    "    # ax1.plot(range(l_edge, r_edge), mark_d_alpha[l_edge:r_edge] * 0.5, label=\"Mark proc (x0.5)\", color=\"black\")\n",
    "    ax1.axhline(0, color=\"red\", linestyle=':', linewidth=0.8)\n",
    "    ax1.axhline((d_alpha_f.std()) * 10, color=\"green\", linestyle=':', linewidth=0.8)\n",
    "    ax1.axhline((- d_alpha_f.std()) * 10, color=\"green\", linestyle=':', linewidth=0.8)\n",
    "    ax1.axhline((np.diff(d_alpha_f).std()) * 100, color=\"blue\", linestyle=':', linewidth=0.8)\n",
    "    ax1.axhline((- np.diff(d_alpha_f).std()) * 100, color=\"blue\", linestyle=':', linewidth=0.8)\n",
    "    # ax1.axhline((meta_da.d_q + meta_da.d_std * meta_da.d_std_bottom) * 10, color=\"green\", linestyle=':', linewidth=0.8)\n",
    "    # ax1.axhline((meta_da.d_q - meta_da.d_std * meta_da.d_std_bottom) * 10, color=\"green\", linestyle=':', linewidth=0.8)\n",
    "    # ax1.axhline((meta_da.d_q + meta_da.d_std * meta_da.d_std_top) * 10, color=\"blue\", linestyle=':', linewidth=0.8)\n",
    "    # ax1.axhline((meta_da.d_q - meta_da.d_std * meta_da.d_std_top) * 10, color=\"blue\", linestyle=':', linewidth=0.8)\n",
    "    ax1.grid(which='major', color='#DDDDDD', linewidth=0.9)\n",
    "    ax1.grid(which='minor', color='#DDDDDD', linestyle=':', linewidth=0.7)\n",
    "    ax1.minorticks_on()\n",
    "    ax1.xaxis.set_minor_locator(AutoMinorLocator(10))\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.plot(range(l_edge, r_edge), sxr[l_edge:r_edge], label=\"SXR\")\n",
    "    # ax2.plot(range(l_edge, r_edge), sxr_d1[l_edge:r_edge], label=\"Diff 1\")\n",
    "    ax2.plot(range(l_edge, r_edge), sxr_f[l_edge:r_edge] * 10, label=\"Filtered D1 (x10)\")\n",
    "    ax2.plot(range(l_edge, r_edge), mark_sxr[l_edge:r_edge] * 0.5, label=\"Mark proc (x0.5)\", color=\"black\")\n",
    "    ax2.axhline(meta_sxr.d_q * 10, color=\"red\", linestyle=':', linewidth=0.7)\n",
    "    ax2.axhline((meta_sxr.d_q + meta_sxr.d_std * meta_sxr.d_std_bottom) * 10, color=\"green\", linestyle=':', linewidth=0.7)\n",
    "    ax2.axhline((meta_sxr.d_q - meta_sxr.d_std * meta_sxr.d_std_bottom) * 10, color=\"green\", linestyle=':', linewidth=0.7)\n",
    "    ax2.axhline((meta_sxr.d_q + meta_sxr.d_std * meta_sxr.d_std_top) * 10, color=\"blue\", linestyle=':', linewidth=0.7)\n",
    "    ax2.axhline((meta_sxr.d_q - meta_sxr.d_std * meta_sxr.d_std_top) * 10, color=\"blue\", linestyle=':', linewidth=0.7)\n",
    "\n",
    "    ax2.grid(which='major', color='#DDDDDD', linewidth=0.9)\n",
    "    ax2.grid(which='minor', color='#DDDDDD', linestyle=':', linewidth=0.7)\n",
    "    ax2.minorticks_on()\n",
    "    ax2.xaxis.set_minor_locator(AutoMinorLocator(10))\n",
    "    ax2.legend()\n",
    "\n",
    "    # mgd_filtered = np.copy(mgd)\n",
    "    # mgd_filtered[abs(mgd_filtered - mgd.mean()) > 5 * mgd.std()] = mgd.mean()\n",
    "    \n",
    "    b, a = signal.butter(5, 0.2)\n",
    "    mgd_f = signal.filtfilt(b, a, mgd)\n",
    "\n",
    "    # mgd_d1 = np.diff(mgd_f)\n",
    "    # mgd_ff = signal.filtfilt(b, a, mgd_d1)\n",
    "\n",
    "    mgd_f_filtered = np.copy(mgd_f)\n",
    "    mgd_f_filtered[abs(mgd_f_filtered - mgd_f.mean()) > 5 * mgd_f.std()] = mgd_f.mean()\n",
    "\n",
    "    mgd_f_d1 = np.diff(mgd_f)\n",
    "    \n",
    "    ax3.plot(range(l_edge, r_edge), mgd[l_edge:r_edge], label=\"MGD abs\")  # semilogy np.diff(sxr)\n",
    "    # ax3.plot(range(l_edge, r_edge), mgd_f_d1[l_edge:r_edge], label=\"Filtered D1 (x10)\")  # semilogy np.diff(sxr)\n",
    "    # ax3.plot(range(l_edge, r_edge), mgd_f_filtered[l_edge:r_edge], label=\"Filtered MGD\")  # semilogy np.diff(sxr)\n",
    "    ax3.axhline(mgd_f.mean(), color=\"red\", linestyle=':', linewidth=0.7)\n",
    "    # ax3.axhline(mgd_ff.mean(), color=\"red\", linestyle=':', linewidth=0.7)\n",
    "    # ax3.axhline((mgd_ff.mean() + 1 * mgd_f_filtered.std()) * 1, color=\"green\", linestyle=':', linewidth=0.7)\n",
    "    # ax3.axhline((mgd_ff.mean() - 1 * mgd_f_filtered.std()) * 1, color=\"green\", linestyle=':', linewidth=0.7)\n",
    "    ax3.axhline((mgd_f.mean() + 1 * mgd.std()) * 1, color=\"blue\", linestyle=':', linewidth=0.7)\n",
    "    ax3.axhline((mgd_f.mean() - 1 * mgd.std()) * 1, color=\"blue\", linestyle=':', linewidth=0.7)\n",
    "    ax3.legend()\n",
    "\n",
    "    ax3.grid(which='major', color='#DDDDDD', linewidth=0.9)\n",
    "    ax3.grid(which='minor', color='#DDDDDD', linestyle=':', linewidth=0.7)\n",
    "    ax3.minorticks_on()\n",
    "    ax3.xaxis.set_minor_locator(AutoMinorLocator(10))\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    # res = list(map(int, input(f\"Input index pairs of ELM fragments ({df.t[l_edge]} - {df.t[r_edge]} ms):\\n\").strip().split()))\n",
    "    # for i in range(0, len(res), 2):\n",
    "    #     board_ind = [res[i], res[i+1]]\n",
    "\n",
    "    #     mark = float(input(\"Input mark to set [ 0 | 1 | 2 ]: \"))\n",
    "        \n",
    "    #     if mode_marking and mark > 0:\n",
    "    #         board_ind = get_borders(df.loc[res[i]:res[i + 1], \"ch1\"].to_numpy(), scale=1.5)\n",
    "    #         board_ind[0] = max(board_ind[0] - step_away + res[i], 0)\n",
    "    #         board_ind[1] += res[i]\n",
    "        \n",
    "    #     df.loc[board_ind[0]:board_ind[1], \"ch1_marked\"] = mark\n",
    "\n",
    "    print(np.argwhere(abs(mgd[l_edge: r_edge] - mgd.mean()) >  mgd.std()) + l_edge)\n",
    "\n",
    "    print(f\"ELM fragment ({df.t[l_edge]} - {df.t[r_edge]} ms)\\n\")\n",
    "    break\n",
    "    # input(f\"ELM fragment ({df.t[l_edge]} - {df.t[r_edge]} ms)\\n\")\n",
    "\n",
    "    plt.close()\n",
    "    plt.clf()\n",
    "    l_edge += step\n",
    "\n",
    "# 217500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T14:35:16.347379Z",
     "start_time": "2024-09-09T14:35:16.344759Z"
    }
   },
   "source": [
    "---\n",
    "### Reporting sync ELM program\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filename = f\"{F_ID}\"\n",
    "\n",
    "sxr_slices = [Slice(0, 0)] + get_slices(mark_sxr)\n",
    "deltas_sync_elm = np.full(shape=len(sxr_slices), fill_value=np.nan)\n",
    "count_desync_elm = np.full(shape=len(sxr_slices), fill_value=np.nan)\n",
    "deltas_desync_elm = np.full(shape=len(sxr_slices), fill_value=np.nan)\n",
    "amplitudes_sync_mgd = np.full(shape=len(sxr_slices), fill_value=np.nan)\n",
    "deltas_sync_mgd = np.full(shape=len(sxr_slices), fill_value=np.nan)\n",
    "amplitudes_desync_mgd = np.full(shape=len(sxr_slices), fill_value=np.nan)\n",
    "deltas_desync_mgd = np.full(shape=len(sxr_slices), fill_value=np.nan)\n",
    "\n",
    "report_lines = []\n",
    "\n",
    "report_lines.append(\"\\n----------------\\n\")\n",
    "report_lines.append(f\"Signal {filename}\\n\")\n",
    "report_lines.append(f\"SXR falls: {len(sxr_slices) - 1}\\n\")\n",
    "report_lines.append(\"-----\\n\")\n",
    "l_shift, r_shift = 100, 1500\n",
    "# logg 6\n",
    "print(\"-\", end=\"\")\n",
    "\n",
    "for sl_i in range(1, len(sxr_slices)):\n",
    "    sxr_pointer = np.argmin(sxr_f[sxr_slices[sl_i].l: sxr_slices[sl_i].r]) + sxr_slices[sl_i].l\n",
    "    \n",
    "    da_slices = get_slices(mark_d_alpha[sxr_slices[sl_i - 1].r: min(sxr_slices[sl_i].r + r_shift, mark_d_alpha.shape[0])])\n",
    "    mgd_slice = sxr_slices[sl_i].copy()\n",
    "    mgd_slice.expand(l_shift)\n",
    "\n",
    "    deltas_sync_mgd[sl_i - 1] = (np.argmax(mgd[mgd_slice.l: mgd_slice.r]) + mgd_slice.l - sxr_pointer) / 1e3\n",
    "    amplitudes_sync_mgd[sl_i - 1] = abs(mgd[mgd_slice.l: mgd_slice.r].max() - mgd.mean())\n",
    "    \n",
    "    if deltas_sync_mgd[sl_i - 1] >= 0.5:\n",
    "        deltas_sync_mgd[sl_i - 1] = np.nan\n",
    "        sync_mgd_info = f\"-- No nearest peaks on MGD\"\n",
    "    else:\n",
    "        sync_mgd_info = f\"-- MGD peak: delta = {deltas_sync_mgd[sl_i - 1]:3.3f} ms, amplitude = {amplitudes_sync_mgd[sl_i - 1]:3.3f}\"\n",
    "\n",
    "    if len(da_slices) == 0:\n",
    "        deltas_sync_elm[sl_i - 1] = np.nan\n",
    "        sync_elm_info = f\"-- No sync ELM on D-alpha\"\n",
    "        desync_elm_info = f\"-- No desync ELM on D-alpha\"  # + \" \" * 15\n",
    "    else:\n",
    "        ind = 0\n",
    "        while sxr_slices[sl_i].r - (da_slices[ind].r + sxr_slices[sl_i - 1].l - l_shift) > 0 and ind + 1 < len(da_slices):\n",
    "            ind += 1\n",
    "\n",
    "        sync_elm_slice = da_slices[ind].copy()\n",
    "        sync_elm_slice.move(sxr_slices[sl_i - 1].l)\n",
    "        deltas_sync_elm[sl_i - 1] = (np.argmax(d_alpha_f[sync_elm_slice.l: sync_elm_slice.r]) + sync_elm_slice.l - sxr_pointer) / 1e3\n",
    "        \n",
    "        if deltas_sync_elm[sl_i - 1] >= 0.5:  #  or deltas_sync_elm[sl_i] < - l_shift / 1e3\n",
    "            deltas_sync_elm[sl_i - 1] = np.nan\n",
    "            sync_elm_info = f\"-- No sync ELM on D-alpha\"\n",
    "        else:\n",
    "            sync_elm_info = f\"-- Sync ELM: d = {deltas_sync_elm[sl_i - 1]:3.3f} ms\"\n",
    "\n",
    "        if ind - 1 <= 0:\n",
    "            count_desync_elm[sl_i - 1] = np.nan\n",
    "            desync_elm_info = f\"-- No desync ELM on D-alpha\"  # + \" \" * 15\n",
    "            desync_mgd_info = \"\"\n",
    "        else:\n",
    "            count_desync_elm[sl_i - 1] = ind - 1\n",
    "            \n",
    "            cur_deltas_desync_elm = np.zeros(int(count_desync_elm[sl_i - 1] - 1))\n",
    "            cur_desync_mgd_amplitude = np.zeros(int(count_desync_elm[sl_i - 1]))\n",
    "            cur_desync_mgd_deltas = np.zeros(int(count_desync_elm[sl_i - 1]))\n",
    "\n",
    "            prev_argmax = np.argmax(d_alpha_f[da_slices[1].l: da_slices[1].r]) + da_slices[1].l\n",
    "            \n",
    "            cur_desync_mgd_amplitude[0] = abs(mgd[da_slices[1].l - 10: da_slices[1].r + 10].max() - mgd.mean())\n",
    "            cur_desync_mgd_deltas[0] = (np.argmax(mgd[da_slices[1].l - 10: da_slices[1].r + 10]) + da_slices[1].l - 10 - prev_argmax) / 1e3\n",
    "            \n",
    "            for elm_ind in range(1, ind - 1):\n",
    "                cur_argmax = np.argmax(d_alpha_f[da_slices[elm_ind].l: da_slices[elm_ind].r]) + da_slices[elm_ind].l\n",
    "                cur_deltas_desync_elm[elm_ind - 1] = (cur_argmax - prev_argmax) / 1e3\n",
    "                \n",
    "                cur_desync_mgd_amplitude[elm_ind] = abs(mgd[da_slices[elm_ind].l - 100: da_slices[elm_ind].r + 100].max() - mgd.mean())\n",
    "                cur_desync_mgd_deltas[elm_ind] = (np.argmax(mgd[da_slices[elm_ind].l - 100: da_slices[elm_ind].r + 100]) + da_slices[elm_ind].l - 100 - cur_argmax) / 1e3\n",
    "                \n",
    "                prev_argmax = cur_argmax\n",
    "\n",
    "            deltas_desync_elm[sl_i - 1] = cur_deltas_desync_elm.mean() if count_desync_elm[sl_i - 1] - 1 > 0 else np.nan\n",
    "            amplitudes_desync_mgd[sl_i - 1] = np.nanmean(cur_desync_mgd_amplitude)\n",
    "            deltas_desync_mgd[sl_i - 1] = np.nanmean(cur_desync_mgd_deltas)\n",
    "            desync_elm_info = f\"-- Desync ELM: count = {count_desync_elm[sl_i - 1]}\"\n",
    "            if count_desync_elm[sl_i - 1] > 1 and cur_deltas_desync_elm.mean() > 1e-6:\n",
    "                desync_elm_info += f\", fr mean = {1 / cur_deltas_desync_elm.mean():3.3f} kGz, fr std = {cur_deltas_desync_elm.std() / (cur_deltas_desync_elm.mean() ** 2):3.3f} kGz\"  # d mean = {desync_elm_deltas.mean():3.3f}, d std = {desync_elm_deltas.std():3.3f} ms, \n",
    "            else:\n",
    "                desync_elm_info += f\", fr mean = nan kGz, fr std = nan kGz\"\n",
    "            desync_mgd_info = f\"-- MGD peaks: deltas mean = {cur_desync_mgd_deltas.mean():3.3f} ms, deltas std = {cur_desync_mgd_deltas.std():3.3f} ms, \"\n",
    "            desync_mgd_info += f\"A mean = {cur_desync_mgd_amplitude.mean():6.6f}, A std = {cur_desync_mgd_amplitude.std():6.6f}\"\n",
    "        \n",
    "    \n",
    "    report_lines.append(f\"{sl_i}. SXR fall - {sxr_pointer / 1e3:3.3f} ms {sync_elm_info}\\n\\t{sync_mgd_info}\\n\\t{desync_elm_info}\\n\\t{desync_mgd_info}\\n\")\n",
    "# logg 8\n",
    "print(\"-\", end=\"\")\n",
    "\n",
    "\n",
    "report_lines.append(\"-----\\n\")\n",
    "report_lines.append(f\"Sync ELM info: deltas mean = {np.nanmean(deltas_sync_elm):.3f} ms, deltas std = {np.nanstd(deltas_sync_elm):.3f} ms\\n\")\n",
    "report_lines.append(f\"Desync ELM info: count mean = {np.nanmean(count_desync_elm):.3f}, count std = {np.nanstd(count_desync_elm):.3f}, \" +\n",
    "                    f\"frequency mean = {1 / np.nanmean(deltas_desync_elm):.3f} ms, frequency std = {np.nanstd(deltas_desync_elm) / (np.nanmean(deltas_desync_elm) ** 2):.3f}\\n\")\n",
    "report_lines.append(f\"MGD peaks info (sync ELM): deltas mean = {np.nanmean(deltas_sync_mgd):.3f} ms, deltas std = {np.nanstd(deltas_sync_mgd):.3f} ms, \" +\n",
    "                    f\"A mean = {np.nanmean(amplitudes_sync_mgd):.3f}, A std = {np.nanstd(amplitudes_sync_mgd):.3f}\\n\")\n",
    "report_lines.append(f\"MGD peaks info (desync ELM): deltas mean = {np.nanmean(deltas_desync_mgd):.3f} ms, \" +\n",
    "                    f\"A mean = {np.nanmean(amplitudes_desync_mgd):.3f}\\n\")\n",
    "report_lines.append(\"-----\\n\")\n",
    "report_lines.append(f\"SXR falls w/o sync ELM in nearest area (-{l_shift * 1e-3} ms; {r_shift * 1e-3} ms): {np.count_nonzero(np.isnan(deltas_sync_elm))}\\n\")\n",
    "report_lines.append(f\"SXR falls w/o peaks on MGD in nearest area (-{l_shift * 1e-3} ms; {l_shift * 1e-3} ms): {np.count_nonzero(np.isnan(deltas_sync_mgd))}\\n\")\n",
    "report_lines.append(\"-----\\n\")\n",
    "report_lines.append(f\"SXR signal info: diff_quantile = {meta_sxr.d_q:.6f}, diff_std = {meta_sxr.d_std:.6f}\\n\")\n",
    "report_lines.append(f\"MGD signal info: mean = {mgd.mean():.6f}, std = {mgd.std():.6f}\\n\")\n",
    "report_lines.append(\"-----\\n\")\n",
    "report_lines.append(f\"SXR diff_std_top_edge: {meta_sxr.d_std_bottom:.3f} (approximate w/ a*exp^b)\\n\")\n",
    "report_lines.append(\"----------------\\n\")\n",
    "# logg 9\n",
    "print(\"-\", end=\"\")\n",
    "\n",
    "print(\"\".join(report_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T14:35:16.569514Z",
     "start_time": "2024-09-09T14:35:16.553867Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def process_fragments(data: np.array, mark_data: np.array, length_edge=10, distance_edge=25, scale=1.5, step_out=10) -> np.array:\n",
    "    proc_slice = Slice(0, 0)\n",
    "    cur_slice = Slice(0, 1)\n",
    "    f_fragment = False\n",
    "\n",
    "    while cur_slice.r < mark_data.shape[0]:\n",
    "        if mark_data[cur_slice.r] == 1.0:\n",
    "            if not f_fragment:\n",
    "                f_fragment = True\n",
    "        elif f_fragment:\n",
    "            # print(start_ind, end_ind)\n",
    "            if scale <= 1:\n",
    "                if not cur_slice.check_length(length_edge):\n",
    "                    mark_data[cur_slice.l: cur_slice.r] = 0.0\n",
    "                elif not proc_slice.collide_slices(cur_slice, distance_edge):\n",
    "                    mark_data[proc_slice.l: proc_slice.r] = 1.0\n",
    "                    proc_slice.copy(cur_slice)\n",
    "            elif scale:\n",
    "                mark_data[cur_slice.l: cur_slice.r] = 0.0\n",
    "                if cur_slice.check_length(length_edge):\n",
    "                    boarders = get_boarders(data[cur_slice.l: cur_slice.r], scale)\n",
    "                    # print(boards)\n",
    "                    boarders[0] = max(boarders[0] + cur_slice.l - step_out, 0)\n",
    "                    boarders[1] = min(boarders[1] + cur_slice.l, mark_data.shape[0])\n",
    "\n",
    "                    mark_data[boarders[0]:boarders[1]] = 1.0\n",
    "\n",
    "            f_fragment = False\n",
    "            cur_slice.collapse_borders()\n",
    "        elif not f_fragment:\n",
    "            cur_slice.collapse_borders()\n",
    "            if proc_slice.is_null():\n",
    "                proc_slice.copy(cur_slice)\n",
    "\n",
    "        cur_slice.step()\n",
    "\n",
    "    return mark_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
