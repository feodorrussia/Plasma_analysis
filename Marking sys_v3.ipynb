{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T13:09:46.601598Z",
     "start_time": "2024-09-09T13:09:43.991844Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shtRipper v1.3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from source.Files_operating import read_sht_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "import random\n",
    "\n",
    "from matplotlib.colors import CSS4_COLORS as COLORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import AutoMinorLocator\n",
    "\n",
    "colors = ['gold', 'brown', 'black', 'seagreen', 'skyblue', 'cyan', 'yellow', 'violet', 'royalblue', 'sandybrown', 'grey', 'gray', 'indigo', 'rosybrown', 'darkviolet', 'coral', 'pink', 'magenta', 'red', 'springgreen', 'darkblue', 'silver', 'seashell', 'green', 'navy', 'purple', 'sienna', 'chocolate', 'orange', 'blue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T14:21:11.463899Z",
     "start_time": "2024-09-09T14:21:11.448103Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def get_boarders(data: np.array, loc_max_ind=None, scale=1.5):\n",
    "    loc_max_ind = np.argmax(data)\n",
    "    dist_ind = np.argsort(np.abs(data - data[loc_max_ind] / scale))\n",
    "    return Slice(dist_ind[dist_ind <= loc_max_ind][0], dist_ind[dist_ind >= loc_max_ind][0])\n",
    "\n",
    "\n",
    "class Slice:\n",
    "    def __init__(self, start_index=0, end_index=0):\n",
    "        self.l = start_index\n",
    "        self.r = end_index\n",
    "        self.mark = 1.0\n",
    "\n",
    "    def set_boarders(self, start_index: int, end_index: int) -> None:\n",
    "        self.l = start_index\n",
    "        self.r = end_index\n",
    "\n",
    "    def set_mark(self, mark: int) -> None:\n",
    "        self.mark = mark\n",
    "\n",
    "    def copy(self):\n",
    "        new_slice = Slice(self.l, self.r)\n",
    "        new_slice.set_mark(new_slice.mark)\n",
    "        return new_slice\n",
    "\n",
    "    def check_length(self, len_edge: int) -> bool:\n",
    "        return self.r - self.l > len_edge\n",
    "\n",
    "    def check_dist(self, other, dist_edge: int) -> bool:\n",
    "        return other.l - self.r > dist_edge\n",
    "\n",
    "    def collide_slices(self, other, dist_edge: int) -> bool:\n",
    "        if not self.check_dist(other, dist_edge):\n",
    "            self.r = other.r\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def step(self) -> None:\n",
    "        self.r += 1\n",
    "\n",
    "    def move(self, delta: int) -> None:\n",
    "        self.r += delta\n",
    "        self.l += delta\n",
    "\n",
    "    def expand(self, delta: int) -> None:\n",
    "        self.r += delta\n",
    "        self.l -= delta\n",
    "\n",
    "    def collapse_boarders(self) -> None:\n",
    "        self.l = self.r\n",
    "\n",
    "    def is_null(self) -> bool:\n",
    "        return self.l == self.r\n",
    "\n",
    "    def to_list(self):\n",
    "        return [self.l, self.r]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"({self.l}, {self.r})\"\n",
    "\n",
    "\n",
    "class Signal_meta:\n",
    "    def __init__(self, chanel_name=\"da\", processing_flag=False, \n",
    "                 quantile_edge=0.0, std_edge=1.0, \n",
    "                 length_edge=10, distance_edge=10, scale=1.5, step_out=10, \n",
    "                 std_bottom_edge=0, std_top_edge=1, d_std_bottom_edge=3, d_std_top_edge=6, amplitude_ratio=0.5):\n",
    "        self.name = chanel_name\n",
    "        self.proc_fl = processing_flag\n",
    "        \n",
    "        self.len_edge = length_edge\n",
    "        self.dist_edge = distance_edge\n",
    "        self.scale = scale\n",
    "        self.step_out = step_out\n",
    "        \n",
    "        self.q = quantile_edge\n",
    "        self.std = std_edge\n",
    "        \n",
    "        self.d_q = quantile_edge\n",
    "        self.d_std = std_edge\n",
    "\n",
    "        self.std_top = std_top_edge\n",
    "        self.std_bottom = std_bottom_edge\n",
    "        self.d_std_top = d_std_top_edge\n",
    "        self.d_std_bottom = d_std_bottom_edge\n",
    "\n",
    "        self.max_min_ratio = amplitude_ratio\n",
    "        \n",
    "    def set_statistics(self, data: np.array, data_diff: np.array, percentile: float, d_percentile: float, std_bottom_edge=0, std_top_edge=1, d_std_bottom_edge=3, d_std_top_edge=6, amplitude_ratio=0.5):\n",
    "        self.q = np.quantile(data, percentile)\n",
    "        self.std = data.std()\n",
    "        self.std_top = std_top_edge\n",
    "        self.std_bottom = std_bottom_edge\n",
    "        \n",
    "        self.d_q = np.quantile(data_diff, percentile)\n",
    "        self.d_std = data_diff.std()\n",
    "        if self.name == \"sxr\":\n",
    "            a = 10.5\n",
    "            b = -850\n",
    "            self.d_std_top = d_std_top_edge / d_std_bottom_edge * a * np.exp(b * self.d_std)\n",
    "            self.d_std_bottom = a * np.exp(b * self.d_std)\n",
    "        else:\n",
    "            self.d_std_top = d_std_top_edge\n",
    "            self.d_std_bottom = d_std_bottom_edge            \n",
    "\n",
    "        self.max_min_ratio = amplitude_ratio\n",
    "\n",
    "    def set_edges(self, length_edge=10, distance_edge=10, scale=1.5, step_out=10):\n",
    "        self.len_edge = length_edge\n",
    "        self.dist_edge = distance_edge\n",
    "        self.scale = scale\n",
    "        self.step_out = step_out\n",
    "\n",
    "\n",
    "def get_peaks(data: np.array, s_i: int) -> np.array:\n",
    "    peaks_ind = []\n",
    "    loc_max = data.min()\n",
    "    m_v = data.mean()\n",
    "    loc_max_ind = 0\n",
    "    increase_fl = False\n",
    "    for i in range(data.shape[0] - 1):\n",
    "        if loc_max < data[i]:\n",
    "            increase_fl = True\n",
    "            loc_max = data[i]\n",
    "            loc_max_ind = i\n",
    "        elif abs((loc_max - data[i]) / (loc_max + 1e-10)) > 0.5 and increase_fl:\n",
    "            if abs(loc_max) > 5 * abs(m_v + 1e-10):\n",
    "                peaks_ind.append(loc_max_ind)\n",
    "            # print(s_i + loc_max_ind, abs((loc_max - data[i]) / loc_max), abs((loc_max - m_v) / m_v))  # len(peaks_ind),\n",
    "            increase_fl = False\n",
    "\n",
    "        if not increase_fl or data[i] < data[i + 1]:\n",
    "            loc_max = data[i]\n",
    "            loc_max_ind = i\n",
    "    return np.array(peaks_ind)\n",
    "\n",
    "def get_boarders_d2(data:np.array, diff_data: np.array, s_i: int, scale=1.5):\n",
    "    d2_data = np.diff(diff_data)\n",
    "    peaks_ind = get_peaks(d2_data, s_i)\n",
    "\n",
    "    if len(peaks_ind) == 0:\n",
    "        return Slice(0, diff_data.shape[0])\n",
    "    if len(peaks_ind) == 1:\n",
    "        scale_slice = get_boarders(data, scale=scale)\n",
    "        # print(scale_slice)\n",
    "        if peaks_ind[0] < diff_data.shape[0] - peaks_ind[0]:\n",
    "            if peaks_ind[0] - scale_slice.r < 0:\n",
    "                return Slice(peaks_ind[0], scale_slice.r)\n",
    "            else:\n",
    "                return Slice(peaks_ind[0], diff_data.shape[0])\n",
    "        else:\n",
    "            if peaks_ind[0] - scale_slice.l > 0:\n",
    "                return Slice(scale_slice.l, peaks_ind[0])\n",
    "            else:\n",
    "                return Slice(0, peaks_ind[0])\n",
    "    return Slice(peaks_ind[0], peaks_ind[-1])\n",
    "\n",
    "def proc_boarders(data: np.array, data_diff: np.array, start_ind: int, scale=1.5) -> Slice:\n",
    "    step = 5\n",
    "    \n",
    "    diff_coeff = 1\n",
    "    if data_diff[start_ind] < 0:\n",
    "        diff_coeff = -1\n",
    "    \n",
    "    cur_ind = (start_ind + diff_coeff * step) if 0 < start_ind + diff_coeff * step < data.shape[0] else start_ind\n",
    "    while 0 < cur_ind + diff_coeff * step < data.shape[0] and data_diff[cur_ind] * data_diff[cur_ind + diff_coeff * step] > 0:\n",
    "        cur_ind += diff_coeff * step\n",
    "\n",
    "    # print(cur_ind - 3 * step, cur_ind + 3 * step, end=\" \")\n",
    "    max_ind = np.argmax(data[max(cur_ind - 3 * step, 0): min(cur_ind + 3 * step, data.shape[0])]) + cur_ind - step\n",
    "    length = max(abs(max_ind - start_ind), 3 * step)\n",
    "    # print(max_ind, length)\n",
    "\n",
    "    # print(max_ind - length, max_ind + 2 * length, end=\" \")\n",
    "    res_slice = get_boarders_d2(data[max(max_ind - length, 0): min(max_ind + 2 * length, data.shape[0])], data_diff[max(max_ind - length, 0): min(max_ind + 2 * length, data.shape[0])], max(max_ind - length, 0), scale=scale)  # get_boarders(data[max_ind - length: max_ind + 2 * length], loc_max_ind=length) | Slice(0, 3 * length)\n",
    "    res_slice.move(max_ind - length)\n",
    "    # print(res_slice.l, res_slice.r)\n",
    "\n",
    "    # add checking diff on right & left boarder (cut on D2 peaks) - done\n",
    "    # add dtw classification (None | ELM | LSO)\n",
    "\n",
    "    # print(res_slice.l, res_slice.r, end=\" \")\n",
    "    # print(abs(data_diff[res_slice.l:res_slice.r].max() - np.quantile(data_diff, 0.7)), data_diff.std())\n",
    "    \n",
    "    return res_slice\n",
    "\n",
    "\n",
    "def proc_slices(mark_data: np.array, data: np.array, data_diff: np.array, meta: Signal_meta) -> np.array:  # data: np.array, , scale=1.5 , step_out=10\n",
    "    proc_slice = Slice(0, 50)\n",
    "    cur_slice = Slice(0, 51)\n",
    "    f_fragment = False\n",
    "\n",
    "    res_mark = np.copy(mark_data)\n",
    "    res_mark[cur_slice.l: cur_slice.r] = 0.0\n",
    "    cur_slice.collapse_boarders()\n",
    "    proc_slice = cur_slice.copy()\n",
    "\n",
    "    c = 0\n",
    "    \n",
    "    while cur_slice.r < res_mark.shape[0]:\n",
    "        if res_mark[cur_slice.r] == 1.0:\n",
    "            if not f_fragment:\n",
    "                f_fragment = True\n",
    "        elif f_fragment:\n",
    "            # print(start_ind, end_ind)\n",
    "            if not cur_slice.check_length(meta.len_edge):\n",
    "                res_mark[cur_slice.l: cur_slice.r] = 0.0\n",
    "            elif not proc_slice.collide_slices(cur_slice, meta.dist_edge):\n",
    "                if meta.proc_fl and meta.scale > 1:\n",
    "                    res_mark[proc_slice.l: proc_slice.r] = 0.0\n",
    "                    start_ind = proc_slice.l if data_diff[proc_slice.l] > 0 else proc_slice.r\n",
    "                    proc_slice = proc_boarders(data, data_diff, start_ind, meta.scale)\n",
    "                    \n",
    "                    cur_slice = Slice(proc_slice.r, proc_slice.r)\n",
    "                \n",
    "                proc_slice.expand(meta.step_out)\n",
    "\n",
    "                if meta.proc_fl and abs(data_diff[proc_slice.l:proc_slice.r].max() - meta.d_q) < meta.d_std_top * meta.d_std and \\\n",
    "                   abs(data_diff[proc_slice.l:proc_slice.r].min() - meta.d_q) < meta.d_std_top * meta.d_std:\n",
    "                    proc_slice.set_mark(0)\n",
    "                \n",
    "                if meta.name == \"sxr\":\n",
    "                    if abs(abs(data_diff[proc_slice.l:proc_slice.r].min()) - meta.d_q) > meta.d_std_top * meta.d_std:\n",
    "                        proc_slice.set_mark(1)\n",
    "                        \n",
    "                    if abs(data_diff[proc_slice.l:proc_slice.r].max() / data_diff[proc_slice.l:proc_slice.r].min()) < meta.max_min_ratio:\n",
    "                        proc_slice.set_mark(1)\n",
    "                    else:\n",
    "                        proc_slice.set_mark(0)\n",
    "                \n",
    "                res_mark[proc_slice.l: proc_slice.r] = proc_slice.mark\n",
    "                c += proc_slice.mark\n",
    "                \n",
    "                proc_slice = cur_slice.copy()\n",
    "            f_fragment = False\n",
    "            cur_slice.collapse_boarders()\n",
    "        elif not f_fragment:\n",
    "            cur_slice.collapse_boarders()\n",
    "            if proc_slice.is_null():\n",
    "                proc_slice = cur_slice.copy()\n",
    "    \n",
    "        cur_slice.step()\n",
    "    # print(c)\n",
    "\n",
    "    return res_mark\n",
    "\n",
    "\n",
    "def get_slices(mark_data: np.array):\n",
    "    cur_slice = Slice(0, 1)\n",
    "    f_fragment = False\n",
    "\n",
    "    slices_list = []\n",
    "    \n",
    "    while cur_slice.r < mark_data.shape[0]:\n",
    "        if mark_data[cur_slice.r] == 1.0:\n",
    "            if not f_fragment:\n",
    "                f_fragment = True\n",
    "        elif f_fragment:\n",
    "            slices_list.append(copy.copy(cur_slice))\n",
    "            f_fragment = False\n",
    "            cur_slice.collapse_boarders()\n",
    "        elif not f_fragment:\n",
    "            cur_slice.collapse_boarders()\n",
    "        cur_slice.step()\n",
    "\n",
    "    return slices_list\n",
    "\n",
    "\n",
    "def get_d2_peaks(diff_data: np.array, mark_data: np.array, s_i: int):\n",
    "    l_ = 0\n",
    "    fr_fl = False\n",
    "    peaks_ind = []\n",
    "    for i in range(mark_data.shape[0]):\n",
    "        if mark_data[i] == 1 and not fr_fl:\n",
    "            fr_fl = True\n",
    "            l_ = i\n",
    "        elif fr_fl and mark_data[i] == 0:\n",
    "            fr_fl = False\n",
    "            d2_data = np.diff(diff_data)\n",
    "            res = get_peaks(d2_data[l_-10:i+30], s_i) + l_-10\n",
    "            peaks_ind += res.tolist()\n",
    "    peaks_ind = np.array(peaks_ind)\n",
    "    return peaks_ind, d2_data[peaks_ind]\n",
    "\n",
    "\n",
    "def zero_bin_search(arr: np.array):\n",
    "    l = 0\n",
    "    r = arr.shape[0] - 1\n",
    "    if arr[l] * arr[r] > 0 or r - l <= 0:\n",
    "        return -1\n",
    "    \n",
    "    while r - l > 1:\n",
    "        m = (r - l) // 2 + l\n",
    "        if arr[l] * arr[m] <= 0:\n",
    "            r = m\n",
    "        else:\n",
    "            l = m\n",
    "    \n",
    "    return r if arr[l] > arr[r] else l\n",
    "\n",
    "\n",
    "def get_d1_crosses(d1_data: np.array, d2_data: np.array, start: int, end: int, d1_coef=1) -> np.array:\n",
    "    d1_std = d1_data.std()\n",
    "    d1_m = d1_data.mean()\n",
    "    d2_std = d2_data.std()\n",
    "    d2_m = d2_data.mean()\n",
    "    \n",
    "    cur_slice = Slice(start, start + 1)\n",
    "    f_slice = False\n",
    "    ans = []\n",
    "    while cur_slice.r < end + 1:\n",
    "        if abs(d1_data[cur_slice.r] - d1_m) < d1_std * d1_coef and d2_data[cur_slice.r] - d2_m < d2_std / 3 * 2:\n",
    "            if not f_slice and (d1_data[cur_slice.r - 1] - d1_m) > d1_std * d1_coef:\n",
    "                f_slice = True\n",
    "        \n",
    "        elif f_slice:  #  and d1_data[cur_slice.r] < 0\n",
    "            # print(cur_slice)\n",
    "            if d1_data[cur_slice.r] > d1_m:\n",
    "                cur_slice.r = np.argmin(d1_data[cur_slice.l: cur_slice.r]) + cur_slice.l\n",
    "                \n",
    "            if d1_data[cur_slice.l] >= d1_m and d1_data[cur_slice.r] <= d1_m:\n",
    "                # print(cur_slice)\n",
    "                zero_i = zero_bin_search(d1_data[cur_slice.l: cur_slice.r])\n",
    "                if zero_i != -1:\n",
    "                    ans.append(zero_i + cur_slice.l)\n",
    "            \n",
    "            f_slice = False\n",
    "            cur_slice.collapse_boarders()\n",
    "            \n",
    "        elif not f_slice:\n",
    "            cur_slice.collapse_boarders()\n",
    "    \n",
    "        cur_slice.step()\n",
    "\n",
    "    if f_slice:\n",
    "        if d1_data[cur_slice.r] > d1_m:\n",
    "            cur_slice.r = np.argmin(d1_data[cur_slice.l: cur_slice.r]) + cur_slice.l\n",
    "            \n",
    "        if d1_data[cur_slice.l] >= d1_m and d1_data[cur_slice.r] <= d1_m:\n",
    "            zero_i = zero_bin_search(d1_data[cur_slice.l: cur_slice.r])\n",
    "            if zero_i != -1:\n",
    "                ans.append(zero_i + cur_slice.l)\n",
    "    return np.array(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_groups(groups):\n",
    "    groups.sort(key=lambda x: len(x), reverse=True)\n",
    "    d_groups = {}\n",
    "    for i in range(len(groups)):\n",
    "        d_groups[i] = True\n",
    "    res_groups = []\n",
    "    i = 0\n",
    "    while i < len(groups):\n",
    "        j = 0\n",
    "        while j < len(groups):  # for every group we check all next groups about subsequention\n",
    "            if i == j:\n",
    "                j += 1\n",
    "                continue\n",
    "            arr1 = groups[i]\n",
    "            arr2 = groups[j]\n",
    "            if arr1[0] >= arr2[0]:  # check that arrays are consistent\n",
    "                j += 1\n",
    "                continue\n",
    "            k = check_subsequention(arr1, arr2)  # get len of subsequention\n",
    "            if k != -1 and k != len(arr2):  # check valide & not full subsequention\n",
    "                d_groups[i] = False\n",
    "                d_groups[j] = False\n",
    "                res_arr = arr1 + arr2[k:]\n",
    "                if not array_equasion(groups[-1], res_arr):\n",
    "                    groups.append(res_arr)\n",
    "                    res_groups.append(res_arr)\n",
    "            j += 1\n",
    "        i += 1\n",
    "\n",
    "    for i in range(len(groups)):\n",
    "        if d_groups[i]:\n",
    "            res_groups.append(groups[i])\n",
    "    return res_groups\n",
    "\n",
    "\n",
    "def check_subsequention(arr1, arr2) -> int:    \n",
    "    n = min(len(arr1), len(arr2))\n",
    "    # print(arr1, arr2, n)\n",
    "    for i in range(1, n + 1):\n",
    "        # print(i, arr1[-i:], arr2[:i])\n",
    "        if arr1[-i] < arr2[0]:\n",
    "            return -1\n",
    "        if array_equasion(arr1[-i:], arr2[:i]):\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "\n",
    "def array_equasion(arr1, arr2) -> bool:\n",
    "    if len(arr1) != len(arr2):\n",
    "        return False\n",
    "    for i in range(len(arr1)):\n",
    "        if arr1[i] != arr2[i]:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def in_array(arr1, arr2) -> bool:\n",
    "    if len(arr1) > len(arr2):\n",
    "        return False\n",
    "    for i in range(len(arr2) - len(arr1) + 1):\n",
    "        if array_equasion(arr1, arr2[i:i + len(arr1)]):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def get_valide_groups_from_struct_by_delta(groups_struct, delta, DELTA_DELTA, MIN_GROUP, DELTA_MAX):  # groups_struct = {Point: {Delta: [mistake: float, group: list]}}\n",
    "    res_groups = []\n",
    "    if delta < DELTA_MAX:\n",
    "        # get groups from delta\n",
    "        for item in groups_struct[delta]:\n",
    "            if len(item[1]) > MIN_GROUP:\n",
    "                res_groups.append(item[1])\n",
    "                \n",
    "        # get groups from delta +- delta_delta \n",
    "        for d2 in groups_struct.keys():\n",
    "            if d != d2 and d2 < DELTA_MAX and abs(d - d2) / d <= DELTA_DELTA:\n",
    "                for item in groups_struct[d2]:\n",
    "                    if len(item[1]) > MIN_GROUP:\n",
    "                        res_groups.append(item[1])\n",
    "    return res_groups\n",
    "\n",
    "\n",
    "def get_valide_groups_from_struct_by_amplitude(groups_struct, d_alpha, peaks):\n",
    "    res_groups = []\n",
    "    g_set = set()\n",
    "    for delta in groups_struct.keys():\n",
    "        for item in groups_struct[delta]:\n",
    "            p_arr = item[1]\n",
    "            n = len(p_arr)\n",
    "            cur_group = [p_arr[0]]\n",
    "            group_m_amplitude = d_alpha[peaks[cur_group]][0]\n",
    "            \n",
    "            if n <= 2 and p_arr[1] != p_arr[0] + 1:  # check valid by consistance on 0 elemnt\n",
    "                continue\n",
    "            \n",
    "            for i in range(1, n):\n",
    "                if i < n - 1 and p_arr[i + 1] != p_arr[i] + 1:  # check valid by consistance on i elemnt\n",
    "                    if abs(d_alpha[peaks[p_arr[i]]] - group_m_amplitude) / max(group_m_amplitude, d_alpha[peaks[p_arr[i]]]) <= 0.34:\n",
    "                        cur_group.append(p_arr[i])\n",
    "                        group_m_amplitude = np.nanmean(d_alpha[peaks[cur_group]])\n",
    "                    break\n",
    "                \n",
    "                if abs(d_alpha[peaks[p_arr[i]]] - group_m_amplitude) / max(group_m_amplitude, d_alpha[peaks[p_arr[i]]]) <= 0.34:\n",
    "                    cur_group.append(p_arr[i])\n",
    "                    group_m_amplitude = np.nanmean(d_alpha[peaks[cur_group]])\n",
    "                elif d_alpha[peaks[p_arr[i]]] > group_m_amplitude:\n",
    "                    if len(cur_group) > 1:\n",
    "                        res_groups.append(cur_group)\n",
    "                    \n",
    "                    cur_group = [p_arr[i]]\n",
    "                    group_m_amplitude = d_alpha[peaks[cur_group]][0]\n",
    "            if len(cur_group) > 0:\n",
    "                group_str = \"/\".join(list(map(str, cur_group)))\n",
    "                if group_str not in g_set:\n",
    "                    g_set.add(group_str)\n",
    "                    res_groups.append(cur_group)\n",
    "                \n",
    "    return res_groups\n",
    "\n",
    "\n",
    "def get_unique_point_from_groups(groups):\n",
    "    set_p = set()\n",
    "    for group in groups:\n",
    "        set_p = set_p.union(set(group))\n",
    "    return sorted(list(set_p))\n",
    "\n",
    "\n",
    "def get_groups_2(arr): # -> groups = {Delta: [mistake: float, group: list]}\n",
    "    n = len(arr)\n",
    "    res_struct = {}\n",
    "    if n < 2:\n",
    "        return res_struct\n",
    "    cur_delta = arr[1] - arr[0]\n",
    "    cur_group = [0, 1]\n",
    "    cur_err = 0.0\n",
    "    for i in range(2, n):\n",
    "        err = abs(arr[i] - (arr[cur_group[-1]] + cur_delta)) - int(random.random() * 10)\n",
    "        if cur_err + err < cur_delta:  # check valide\n",
    "            cur_err += err\n",
    "            cur_group.append(i)\n",
    "        else:  # save cur group & upd pointer\n",
    "            if cur_delta in res_struct.keys():\n",
    "                res_struct[cur_delta].append([cur_err, cur_group])\n",
    "                res_struct[cur_delta].sort(key=lambda x: len(x[1]), reverse=True)\n",
    "            else:\n",
    "                res_struct[cur_delta] = [[cur_err, cur_group]]\n",
    "            \n",
    "            cur_delta = arr[i] - arr[cur_group[-1]]\n",
    "            cur_group = [cur_group[-1], i]\n",
    "            cur_err = 0.0\n",
    "    \n",
    "    if cur_delta in res_struct.keys():\n",
    "        res_struct[cur_delta].append([cur_err, cur_group])\n",
    "        res_struct[cur_delta].sort(key=lambda x: len(x[1]), reverse=True)\n",
    "    else:\n",
    "        res_struct[cur_delta] = [[cur_err, cur_group]]\n",
    "    return res_struct\n",
    "\n",
    "\n",
    "def get_groups(arr): # -> groups = {Delta: [mistake: float, group: list]}\n",
    "    n = len(arr)\n",
    "    res_struct = {}\n",
    "    if n < 2:\n",
    "        return res_struct\n",
    "    cur_group = []\n",
    "\n",
    "    stac_p =  list(range(n - 2, -1, -1))\n",
    "    while len(stac_p) > 0:\n",
    "        # print(stac_p, cur_group)\n",
    "        ind = stac_p.pop()\n",
    "        # print(ind)\n",
    "        fl_save = False\n",
    "        if len(cur_group) == 0:\n",
    "            cur_delta = arr[ind + 1] - arr[ind]\n",
    "            cur_group = [ind, ind + 1]\n",
    "            cur_err = 0.0\n",
    "\n",
    "            if ind < n - 2:\n",
    "                stac_p.append(ind + 2)\n",
    "            else:\n",
    "                fl_save = True\n",
    "        else:\n",
    "            while ind < n - 2 and abs(arr[ind] - (arr[cur_group[-1]] + cur_delta)) >= abs(arr[ind + 1] - (arr[cur_group[-1]] + cur_delta)):\n",
    "                if ind not in stac_p:\n",
    "                    stac_p.append(ind)\n",
    "                ind += 1\n",
    "\n",
    "            err = abs(arr[ind] - (arr[cur_group[-1]] + cur_delta))\n",
    "            if cur_err + err < cur_delta:  # check valide\n",
    "                cur_err += err\n",
    "                cur_group.append(ind)\n",
    "                if ind < n - 1:\n",
    "                    stac_p.append(ind + 1)\n",
    "                else:\n",
    "                    fl_save = True\n",
    "            else:  # save cur group & upd pointer\n",
    "                fl_save = True\n",
    "                stac_p.append(ind - 1)\n",
    "    \n",
    "        if fl_save or ind == n - 1:\n",
    "            if cur_delta in res_struct.keys():\n",
    "                res_struct[cur_delta].append([cur_err, cur_group])\n",
    "                res_struct[cur_delta].sort(key=lambda x: len(x[1]), reverse=True)\n",
    "            else:\n",
    "                res_struct[cur_delta] = [[cur_err, cur_group]]\n",
    "            cur_group = []\n",
    "        \n",
    "    return res_struct\n",
    "\n",
    "\n",
    "def get_time_delta(arr):\n",
    "    return (arr[-1] - arr[0]) / (len(arr) - 1)\n",
    "\n",
    "\n",
    "def merge_peaks(points, d_alpha):\n",
    "    d = get_time_delta(points)\n",
    "    res = [points[0]]\n",
    "    for i in range(1, len(points)):\n",
    "        if (points[i] - res[-1]) / d < 0.7:\n",
    "            res[-1] = points[i] if d_alpha[points[i]] > d_alpha[points[i - 1]] else points[i - 1]\n",
    "        else:\n",
    "            res.append(points[i])\n",
    "    return res\n",
    "\n",
    "def get_groups_from_signal(d_alpha, d_alpha_f, d_alpha_d2f, l_edge, r_edge):  # -> groups = [group: list(time points)]\n",
    "    DELTA_DELTA = 0.1 # \n",
    "    DELTA_MAX = 1000 # points\n",
    "    MIN_GROUP = 2 # min num points in group\n",
    "\n",
    "    # get peaks on diagnostic (in the one group)\n",
    "    peaks = np.array(get_d1_crosses(d_alpha_f, d_alpha_d2f, l_edge, r_edge))\n",
    "    # m_, std_ = np.mean(d_alpha[pre_peaks]), np.std(d_alpha[pre_peaks])\n",
    "    # peaks_ind = np.argwhere((m_ - d_alpha[pre_peaks]) / std_ < 1).transpose()[0]  # ((m_ - d_alpha[pre_peaks]) / std_ < 1) | ((m_ - d_alpha[pre_peaks]) / std_ > 10)\n",
    "    # peaks = pre_peaks[peaks_ind]\n",
    "    # print(peaks_ind)\n",
    "    # print((m_ - d_alpha[peaks]) / std_)\n",
    "\n",
    "    # check amplitude\n",
    "\n",
    "    # # divide peaks to groups\n",
    "    prev_peaks = []\n",
    "    cur_peaks = copy.copy(peaks)\n",
    "    while not array_equasion(prev_peaks, cur_peaks):\n",
    "        # print(\"- logg: \", cur_peaks)\n",
    "        # save peaks time points\n",
    "        prev_peaks = copy.copy(cur_peaks)\n",
    "        # get struct of groups = {Delta: [mistake: float, group: list]}\n",
    "        all_groups_struct = get_groups(cur_peaks)\n",
    "        # print(\"- logg: \", all_groups_struct)\n",
    "        # get list of valid groups by amplitude\n",
    "        valid_groups = get_valide_groups_from_struct_by_amplitude(all_groups_struct, d_alpha, cur_peaks)\n",
    "        valid_groups = list(map(lambda x: merge_peaks(x, d_alpha) if len(x) > 1 else x, valid_groups))\n",
    "        # print(\"- logg: \", valid_groups)\n",
    "        # get list of unique peaks indeces\n",
    "        peaks_ind = get_unique_point_from_groups(valid_groups)\n",
    "        # get new peaks time points\n",
    "        cur_peaks = copy.copy(cur_peaks[peaks_ind])\n",
    "    \n",
    "    all_groups_struct = get_groups(cur_peaks)\n",
    "    # print(\"- logg: \", all_groups_struct)\n",
    "    valid_groups = get_valide_groups_from_struct_by_amplitude(all_groups_struct, d_alpha, cur_peaks)\n",
    "    # print(\"- logg: \", sorted(valid_groups, key=lambda x: x[0]))\n",
    "    \n",
    "    # # group post processing: union by delta & get missing\n",
    "    # res_groups = []\n",
    "    # for d in sorted(all_groups_struct.keys()):\n",
    "    #     # get all groups from d & d+-delta_delta\n",
    "    #     delta_groups = get_valide_groups_from_struct_by_delta(all_groups_struct, d, DELTA_DELTA, MIN_GROUP, DELTA_MAX)\n",
    "    #     # combine subsequent groups\n",
    "    #     res_groups += combine_groups(delta_groups)\n",
    "    valid_groups = list(map(lambda x: merge_peaks(x, d_alpha) if len(x) > 1 else x, valid_groups))\n",
    "    valid_groups = list(filter(lambda x: len(x) > 1, sorted(valid_groups, key=lambda x: x[0])))\n",
    "    res_peaks = [cur_peaks[valid_groups[0]]]\n",
    "    for gr_indeces in valid_groups[1:]:\n",
    "        d1, d2 = get_time_delta(cur_peaks[gr_indeces]), get_time_delta(res_peaks[-1])\n",
    "        # print(\"- logg: \", res_peaks[-1], gr_indeces, abs(d1 - d2) / d1)\n",
    "        if (0.3 < abs(d1 - d2) / d1 < 0.5 and d1 >= d2) or (0.3 < abs(d1 - d2) / d1 < 1.2 and d1 < d2):\n",
    "            # print(\"- logg: \", res_peaks[-1], gr_indeces, abs(d1 - d2) / d1)\n",
    "            # res_peaks.append(merge_peaks(cur_peaks[gr_indeces], d_alpha))\n",
    "            res_peaks.append(cur_peaks[gr_indeces])\n",
    "            continue\n",
    "        \n",
    "        k = check_subsequention(res_peaks[-1], cur_peaks[gr_indeces])  # get len of subsequention\n",
    "        if k != -1 and k != len(cur_peaks[gr_indeces]):  # check valide & not full subsequention\n",
    "            res_arr = np.concatenate([res_peaks[-1], cur_peaks[gr_indeces][k:]])\n",
    "            # print(\"- logg: \", res_peaks[-1], cur_peaks[gr_indeces], res_arr)\n",
    "            # res_peaks[-1] = copy.copy(merge_peaks(res_arr, d_alpha))\n",
    "            res_peaks[-1] = copy.copy(merge_peaks(res_arr, d_alpha))\n",
    "        elif in_array(res_peaks[-1], cur_peaks[gr_indeces]):\n",
    "            # print(\"- logg: \", res_peaks[-1], cur_peaks[gr_indeces])\n",
    "            # res_peaks[-1] = copy.copy(merge_peaks(cur_peaks[gr_indeces], d_alpha))\n",
    "            res_peaks[-1] = copy.copy(cur_peaks[gr_indeces])\n",
    "        elif k != len(cur_peaks[gr_indeces]) and not in_array(cur_peaks[gr_indeces], res_peaks[-1]):\n",
    "            # print(\"- logg: \", k, gr_indeces)\n",
    "            # res_peaks.append(merge_peaks(cur_peaks[gr_indeces], d_alpha))\n",
    "            res_peaks.append(cur_peaks[gr_indeces])\n",
    "    return res_peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T13:10:12.772791Z",
     "start_time": "2024-09-09T13:10:07.640437Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t</th>\n",
       "      <th>d_alpha</th>\n",
       "      <th>sxr</th>\n",
       "      <th>mgd_v</th>\n",
       "      <th>mgd_r</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>393216.000000</td>\n",
       "      <td>393216.000000</td>\n",
       "      <td>393216.000000</td>\n",
       "      <td>393216.000000</td>\n",
       "      <td>393216.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.196608</td>\n",
       "      <td>0.281714</td>\n",
       "      <td>0.386184</td>\n",
       "      <td>-0.002304</td>\n",
       "      <td>-0.006000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.113512</td>\n",
       "      <td>0.393401</td>\n",
       "      <td>0.503047</td>\n",
       "      <td>0.085549</td>\n",
       "      <td>0.096789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.388535</td>\n",
       "      <td>-0.076685</td>\n",
       "      <td>-5.235000</td>\n",
       "      <td>-5.235000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.098304</td>\n",
       "      <td>0.012781</td>\n",
       "      <td>0.107358</td>\n",
       "      <td>-0.005112</td>\n",
       "      <td>-0.010225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.196607</td>\n",
       "      <td>0.066460</td>\n",
       "      <td>0.109915</td>\n",
       "      <td>-0.002556</td>\n",
       "      <td>-0.005112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.294911</td>\n",
       "      <td>0.575134</td>\n",
       "      <td>0.439658</td>\n",
       "      <td>0.002556</td>\n",
       "      <td>-0.002556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.393215</td>\n",
       "      <td>4.818347</td>\n",
       "      <td>2.356772</td>\n",
       "      <td>5.232444</td>\n",
       "      <td>5.232444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   t        d_alpha            sxr          mgd_v  \\\n",
       "count  393216.000000  393216.000000  393216.000000  393216.000000   \n",
       "mean        0.196608       0.281714       0.386184      -0.002304   \n",
       "std         0.113512       0.393401       0.503047       0.085549   \n",
       "min         0.000000      -0.388535      -0.076685      -5.235000   \n",
       "25%         0.098304       0.012781       0.107358      -0.005112   \n",
       "50%         0.196607       0.066460       0.109915      -0.002556   \n",
       "75%         0.294911       0.575134       0.439658       0.002556   \n",
       "max         0.393215       4.818347       2.356772       5.232444   \n",
       "\n",
       "               mgd_r  \n",
       "count  393216.000000  \n",
       "mean       -0.006000  \n",
       "std         0.096789  \n",
       "min        -5.235000  \n",
       "25%        -0.010225  \n",
       "50%        -0.005112  \n",
       "75%        -0.002556  \n",
       "max         5.232444  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F_ID = 44184\n",
    "proj_path = \"D:/Edu/Lab/Projects/Plasma_analysis\"\n",
    "# D:/Edu/Lab/Projects/Plasma_analysis | C:/Users/f.belous/Work/Projects/Plasma_analysis\n",
    "dir_path = proj_path + \"/data/sht/G-ELM/\"  \n",
    "\n",
    "df = read_sht_data(f'sht{F_ID}', dir_path)\n",
    "df = df.rename(columns={\"ch1\": \"d_alpha\"})\n",
    "df[\"sxr\"] = read_sht_data(f'sht{F_ID}', dir_path, data_name=\"SXR 50 mkm\").ch1\n",
    "# dbs = read_dataFile(f'data/dbs/{F_ID} DBS.dat')\n",
    "# mgd_data_tor\n",
    "# mgd_data_vertical\n",
    "df[\"mgd_v\"] = read_sht_data(f'sht{F_ID}', dir_path, data_name=\"МГД быстрый зонд верт.\").ch1\n",
    "# mgd_data_radial\n",
    "df[\"mgd_r\"] = read_sht_data(f'sht{F_ID}', dir_path, data_name=\"МГД быстрый зонд рад.\").ch1\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T13:10:17.649171Z",
     "start_time": "2024-09-09T13:10:17.628135Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "d_alpha = df.d_alpha.to_numpy()\n",
    "sxr = df.sxr.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T13:10:18.100388Z",
     "start_time": "2024-09-09T13:10:18.068168Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "d_alpha_d1 = np.diff(d_alpha)\n",
    "sxr_d1 = np.diff(sxr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "mgd = df.mgd_v.to_numpy() ** 2 + df.mgd_r.to_numpy() ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T13:10:26.340302Z",
     "start_time": "2024-09-09T13:10:22.341690Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "from source.NN_environment import normalise_series\n",
    "\n",
    "# def normalise_series(data):\n",
    "#     max_value, min_value, mean_value = np.max(data), np.min(data), np.mean(data)\n",
    "#     return (data - mean_value) / abs(max_value - min_value)\n",
    "\n",
    "def filt_signal(arr, N, W):\n",
    "    b, a = signal.butter(N, W)\n",
    "    return signal.filtfilt(b, a, arr)\n",
    "\n",
    "def smooth(y, box_pts):\n",
    "    x = np.arange(box_pts) / box_pts * 2 - 1\n",
    "    box = 1 / (2 * np.pi) ** 0.5 * np.exp(-x ** 2 / 2)\n",
    "    y_smooth = np.convolve(y, box, mode='same')\n",
    "    return y_smooth\n",
    "\n",
    "d_alpha_f = filt_signal(d_alpha_d1, 5, 0.1)\n",
    "sxr_f = filt_signal(sxr_d1, 5, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t</th>\n",
       "      <th>ch1</th>\n",
       "      <th>ch2</th>\n",
       "      <th>ch3</th>\n",
       "      <th>ch4</th>\n",
       "      <th>ch5</th>\n",
       "      <th>ch6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>512000.000000</td>\n",
       "      <td>512000.000000</td>\n",
       "      <td>512000.000000</td>\n",
       "      <td>512000.000000</td>\n",
       "      <td>512000.000000</td>\n",
       "      <td>512000.000000</td>\n",
       "      <td>512000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.164000</td>\n",
       "      <td>0.056893</td>\n",
       "      <td>0.150608</td>\n",
       "      <td>0.156078</td>\n",
       "      <td>0.136923</td>\n",
       "      <td>0.150784</td>\n",
       "      <td>0.150296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.036951</td>\n",
       "      <td>0.306641</td>\n",
       "      <td>0.692173</td>\n",
       "      <td>0.190068</td>\n",
       "      <td>0.190111</td>\n",
       "      <td>0.043122</td>\n",
       "      <td>0.038470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>-1.636719</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>-1.193359</td>\n",
       "      <td>-1.324219</td>\n",
       "      <td>-0.167969</td>\n",
       "      <td>-0.136719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.132000</td>\n",
       "      <td>-0.068359</td>\n",
       "      <td>-0.126953</td>\n",
       "      <td>0.074219</td>\n",
       "      <td>0.056641</td>\n",
       "      <td>0.128906</td>\n",
       "      <td>0.128906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.164000</td>\n",
       "      <td>0.060547</td>\n",
       "      <td>0.152344</td>\n",
       "      <td>0.158203</td>\n",
       "      <td>0.136719</td>\n",
       "      <td>0.150391</td>\n",
       "      <td>0.150391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.196000</td>\n",
       "      <td>0.173828</td>\n",
       "      <td>0.423828</td>\n",
       "      <td>0.234375</td>\n",
       "      <td>0.216797</td>\n",
       "      <td>0.171875</td>\n",
       "      <td>0.171875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.228000</td>\n",
       "      <td>2.519531</td>\n",
       "      <td>3.998047</td>\n",
       "      <td>1.802734</td>\n",
       "      <td>1.533203</td>\n",
       "      <td>0.521484</td>\n",
       "      <td>0.529297</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   t            ch1            ch2            ch3  \\\n",
       "count  512000.000000  512000.000000  512000.000000  512000.000000   \n",
       "mean        0.164000       0.056893       0.150608       0.156078   \n",
       "std         0.036951       0.306641       0.692173       0.190068   \n",
       "min         0.100000      -1.636719      -4.000000      -1.193359   \n",
       "25%         0.132000      -0.068359      -0.126953       0.074219   \n",
       "50%         0.164000       0.060547       0.152344       0.158203   \n",
       "75%         0.196000       0.173828       0.423828       0.234375   \n",
       "max         0.228000       2.519531       3.998047       1.802734   \n",
       "\n",
       "                 ch4            ch5            ch6  \n",
       "count  512000.000000  512000.000000  512000.000000  \n",
       "mean        0.136923       0.150784       0.150296  \n",
       "std         0.190111       0.043122       0.038470  \n",
       "min        -1.324219      -0.167969      -0.136719  \n",
       "25%         0.056641       0.128906       0.128906  \n",
       "50%         0.136719       0.150391       0.150391  \n",
       "75%         0.216797       0.171875       0.171875  \n",
       "max         1.533203       0.521484       0.529297  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbs_dir_path = proj_path + \"/data/dbs/sht/\"\n",
    "dbs_df = read_sht_data(f'Dref{F_ID}', dbs_dir_path, data_name=\"ch1\")\n",
    "channels = [1, 2, 3, 4, 5, 6]\n",
    "for i in channels[1:]:\n",
    "    dbs_df[f\"ch{i}\"] = read_sht_data(f'Dref{F_ID}', dbs_dir_path, data_name=f\"ch{i}\").ch1\n",
    "\n",
    "dbs_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t</th>\n",
       "      <th>ch1</th>\n",
       "      <th>ch2</th>\n",
       "      <th>ch3</th>\n",
       "      <th>ch4</th>\n",
       "      <th>ch5</th>\n",
       "      <th>ch6</th>\n",
       "      <th>ch1_A</th>\n",
       "      <th>ch1_dfi</th>\n",
       "      <th>ch3_A</th>\n",
       "      <th>ch3_dfi</th>\n",
       "      <th>ch5_A</th>\n",
       "      <th>ch5_dfi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>512000.000000</td>\n",
       "      <td>512000.000000</td>\n",
       "      <td>512000.000000</td>\n",
       "      <td>512000.000000</td>\n",
       "      <td>512000.000000</td>\n",
       "      <td>512000.000000</td>\n",
       "      <td>512000.000000</td>\n",
       "      <td>512000.000000</td>\n",
       "      <td>5.120000e+05</td>\n",
       "      <td>512000.000000</td>\n",
       "      <td>5.120000e+05</td>\n",
       "      <td>512000.000000</td>\n",
       "      <td>512000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.164000</td>\n",
       "      <td>0.056893</td>\n",
       "      <td>0.150608</td>\n",
       "      <td>0.156078</td>\n",
       "      <td>0.136923</td>\n",
       "      <td>0.150784</td>\n",
       "      <td>0.150296</td>\n",
       "      <td>0.036727</td>\n",
       "      <td>3.805887e-03</td>\n",
       "      <td>0.016120</td>\n",
       "      <td>3.680114e-03</td>\n",
       "      <td>0.004834</td>\n",
       "      <td>-0.000119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.036951</td>\n",
       "      <td>0.306641</td>\n",
       "      <td>0.692173</td>\n",
       "      <td>0.190068</td>\n",
       "      <td>0.190111</td>\n",
       "      <td>0.043122</td>\n",
       "      <td>0.038470</td>\n",
       "      <td>0.194435</td>\n",
       "      <td>8.555648e-02</td>\n",
       "      <td>0.154039</td>\n",
       "      <td>9.483878e-02</td>\n",
       "      <td>0.134205</td>\n",
       "      <td>0.083499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>-1.636719</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>-1.193359</td>\n",
       "      <td>-1.324219</td>\n",
       "      <td>-0.167969</td>\n",
       "      <td>-0.136719</td>\n",
       "      <td>-0.239648</td>\n",
       "      <td>-4.534209e-01</td>\n",
       "      <td>-0.211384</td>\n",
       "      <td>-4.516469e-01</td>\n",
       "      <td>-0.248822</td>\n",
       "      <td>-0.557085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.132000</td>\n",
       "      <td>-0.068359</td>\n",
       "      <td>-0.126953</td>\n",
       "      <td>0.074219</td>\n",
       "      <td>0.056641</td>\n",
       "      <td>0.128906</td>\n",
       "      <td>0.128906</td>\n",
       "      <td>-0.117943</td>\n",
       "      <td>-4.142156e-02</td>\n",
       "      <td>-0.109644</td>\n",
       "      <td>-5.241647e-02</td>\n",
       "      <td>-0.100075</td>\n",
       "      <td>-0.047127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.164000</td>\n",
       "      <td>0.060547</td>\n",
       "      <td>0.152344</td>\n",
       "      <td>0.158203</td>\n",
       "      <td>0.136719</td>\n",
       "      <td>0.150391</td>\n",
       "      <td>0.150391</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.169198e-19</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.471219e-19</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.196000</td>\n",
       "      <td>0.173828</td>\n",
       "      <td>0.423828</td>\n",
       "      <td>0.234375</td>\n",
       "      <td>0.216797</td>\n",
       "      <td>0.171875</td>\n",
       "      <td>0.171875</td>\n",
       "      <td>0.175749</td>\n",
       "      <td>4.526520e-02</td>\n",
       "      <td>0.116560</td>\n",
       "      <td>5.553408e-02</td>\n",
       "      <td>0.093555</td>\n",
       "      <td>0.047572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.228000</td>\n",
       "      <td>2.519531</td>\n",
       "      <td>3.998047</td>\n",
       "      <td>1.802734</td>\n",
       "      <td>1.533203</td>\n",
       "      <td>0.521484</td>\n",
       "      <td>0.529297</td>\n",
       "      <td>0.760352</td>\n",
       "      <td>5.465791e-01</td>\n",
       "      <td>0.788616</td>\n",
       "      <td>5.483531e-01</td>\n",
       "      <td>0.751178</td>\n",
       "      <td>0.442915</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   t            ch1            ch2            ch3  \\\n",
       "count  512000.000000  512000.000000  512000.000000  512000.000000   \n",
       "mean        0.164000       0.056893       0.150608       0.156078   \n",
       "std         0.036951       0.306641       0.692173       0.190068   \n",
       "min         0.100000      -1.636719      -4.000000      -1.193359   \n",
       "25%         0.132000      -0.068359      -0.126953       0.074219   \n",
       "50%         0.164000       0.060547       0.152344       0.158203   \n",
       "75%         0.196000       0.173828       0.423828       0.234375   \n",
       "max         0.228000       2.519531       3.998047       1.802734   \n",
       "\n",
       "                 ch4            ch5            ch6          ch1_A  \\\n",
       "count  512000.000000  512000.000000  512000.000000  512000.000000   \n",
       "mean        0.136923       0.150784       0.150296       0.036727   \n",
       "std         0.190111       0.043122       0.038470       0.194435   \n",
       "min        -1.324219      -0.167969      -0.136719      -0.239648   \n",
       "25%         0.056641       0.128906       0.128906      -0.117943   \n",
       "50%         0.136719       0.150391       0.150391       0.000000   \n",
       "75%         0.216797       0.171875       0.171875       0.175749   \n",
       "max         1.533203       0.521484       0.529297       0.760352   \n",
       "\n",
       "            ch1_dfi          ch3_A       ch3_dfi          ch5_A        ch5_dfi  \n",
       "count  5.120000e+05  512000.000000  5.120000e+05  512000.000000  512000.000000  \n",
       "mean   3.805887e-03       0.016120  3.680114e-03       0.004834      -0.000119  \n",
       "std    8.555648e-02       0.154039  9.483878e-02       0.134205       0.083499  \n",
       "min   -4.534209e-01      -0.211384 -4.516469e-01      -0.248822      -0.557085  \n",
       "25%   -4.142156e-02      -0.109644 -5.241647e-02      -0.100075      -0.047127  \n",
       "50%    2.169198e-19       0.000000  2.471219e-19       0.000000       0.000000  \n",
       "75%    4.526520e-02       0.116560  5.553408e-02       0.093555       0.047572  \n",
       "max    5.465791e-01       0.788616  5.483531e-01       0.751178       0.442915  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for ind in range(1, len(channels) + 1, 2):\n",
    "    w = 0.1\n",
    "    smooth_lenght = 100\n",
    "    i_data = normalise_series(dbs_df[f\"ch{ind}\"].to_numpy())  # filt_signal( , 5, w)\n",
    "    q_data = normalise_series(dbs_df[f\"ch{ind + 1}\"].to_numpy())\n",
    "\n",
    "    c_data = i_data + q_data*1j\n",
    "\n",
    "    dbs_df[f\"ch{ind}_A\"] = normalise_series(smooth(filt_signal(np.abs(c_data), 5, w), smooth_lenght))\n",
    "    dbs_df[f\"ch{ind}_dfi\"] = normalise_series(smooth(filt_signal(np.concatenate([np.diff(smooth(filt_signal(np.angle(c_data) , 5, w), smooth_lenght)), [0]]) , 5, w), smooth_lenght))  # smooth(np.concatenate([filt_signal(np.diff(np.angle(c_data)), 5, w), [0]]), 300)\n",
    "\n",
    "dbs_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arr = []  # l_edge, r_edge, n, fr, fr_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_sxr = Signal_meta(chanel_name=\"sxr\", processing_flag=True)\n",
    "meta_sxr.set_statistics(sxr, sxr_f, 0.8, 0.8, d_std_bottom_edge=7., d_std_top_edge=13.0)\n",
    "meta_sxr.set_edges(length_edge=5, distance_edge=30, scale=0, step_out=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T14:59:43.298141Z",
     "start_time": "2024-09-09T14:59:42.537753Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mark_data = np.zeros(sxr_f.shape)\n",
    "mark_data[abs(sxr_f - meta_sxr.d_q) > meta_sxr.d_std * meta_sxr.d_std_bottom] = 1\n",
    "mark_sxr = proc_slices(mark_data, sxr, sxr_f, meta_sxr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[175528 178528]\n",
      " [178528 180528]\n",
      " [180528 182528]\n",
      " [182528 185691]\n",
      " [185872 189111]\n",
      " [189326 192968]\n",
      " [193150 196515]\n",
      " [196698 199847]\n",
      " [200046 203590]\n",
      " [203801 208077]\n",
      " [208296 212556]\n",
      " [212763 215904]\n",
      " [216086 219157]\n",
      " [219364 222135]\n",
      " [222321 224995]\n",
      " [225200 228011]\n",
      " [228217 231020]\n",
      " [231209 233392]\n",
      " [233579 235508]\n",
      " [235687 237929]\n",
      " [238128 240160]\n",
      " [240333 241934]\n",
      " [242111 243642]\n",
      " [243813 245122]\n",
      " [245285 246439]\n",
      " [246590 251279]\n",
      " [251432 256033]\n",
      " [256151 257101]]\n"
     ]
    }
   ],
   "source": [
    "# get slices from standart sxr proccessing\n",
    "sxr_slices = get_slices(mark_sxr)  # [Slice(0, 0)] + \n",
    "slices_edges = []\n",
    "step_out = 50\n",
    "\n",
    "len_top = 5000\n",
    "len_width = 3000\n",
    "len_step = 2000\n",
    "\n",
    "# get slices btw sxr falls\n",
    "slices_edges.append([sxr_slices[0].r + step_out, 0])\n",
    "for i in range(1, len(sxr_slices)):\n",
    "    cur_l_edge = slices_edges[-1][0]\n",
    "    while sxr_slices[i].l - step_out - cur_l_edge > len_top:\n",
    "        slices_edges[-1][1] = cur_l_edge + len_width\n",
    "        slices_edges.append([cur_l_edge + len_width, 0])\n",
    "        cur_l_edge += len_step\n",
    "    \n",
    "    slices_edges[-1][1] = sxr_slices[i].l - step_out\n",
    "    slices_edges.append([sxr_slices[i].r + step_out, 0])\n",
    "\n",
    "slices_edges[-1][1] = min(sxr_slices[-1].r + 1000, sxr.shape[0] - step_out)\n",
    "slices_edges = np.array(slices_edges)\n",
    "print(slices_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 175479)\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "# proccess Slice(0, 1st sxr fall)\n",
    "first_slice = Slice(150, sxr_slices[0].l + step_out)\n",
    "print(first_slice)\n",
    "\n",
    "d_alpha_slice = d_alpha[first_slice.l: first_slice.r]\n",
    "d_alpha_f_slice = d_alpha_f[first_slice.l: first_slice.r]\n",
    "\n",
    "meta_da = Signal_meta(chanel_name=\"da\", processing_flag=True)\n",
    "meta_da.set_statistics(d_alpha, d_alpha_f, 0.7, 0.7, d_std_bottom_edge=2., d_std_top_edge=1.0)\n",
    "meta_da.set_edges(length_edge=50, distance_edge=100)\n",
    "\n",
    "mark_data = np.zeros(d_alpha_f_slice.shape)\n",
    "mark_data[abs(d_alpha_f_slice - meta_da.d_q) > meta_da.d_std * meta_da.d_std_bottom] = 1\n",
    "# mark_d_alpha = proc_slices(mark_data, d_alpha_slice, d_alpha_f_slice, meta_da)\n",
    "\n",
    "da_slices = get_slices(mark_data)  # list(filter(lambda x: x.check_length(10), get_slices(mark_d_alpha)))\n",
    "\n",
    "first_slice_edges = []\n",
    "dist_top = 1000\n",
    "step_out = 100\n",
    "\n",
    "first_slice_edges.append([max(0, da_slices[0].l - step_out), da_slices[0].r + step_out])\n",
    "for i in range(len(da_slices)):\n",
    "    if da_slices[i].l - first_slice_edges[-1][1] > dist_top:\n",
    "        first_slice_edges.append([da_slices[i].l - step_out, da_slices[i].l])\n",
    "    elif first_slice_edges[-1][1] - first_slice_edges[-1][0] > len_top:\n",
    "        first_slice_edges.append([first_slice_edges[-1][1] - step_out, first_slice_edges[-1][1]])\n",
    "    first_slice_edges[-1][1] = da_slices[i].r + step_out\n",
    "        \n",
    "\n",
    "first_slice_edges = np.array(first_slice_edges) + 150\n",
    "print(len(first_slice_edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n"
     ]
    }
   ],
   "source": [
    "res_slices_edges = np.concatenate([first_slice_edges, slices_edges]).astype(np.int64)  # slices_edges.astype(np.int64)\n",
    "\n",
    "slices_marks = np.zeros(len(res_slices_edges))\n",
    "print(len(res_slices_edges))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_ind = 9  # 0\n",
    "\n",
    "for ind in range(start_ind, res_slices_edges.shape[0]):\n",
    "    l_edge, r_edge = res_slices_edges[ind]\n",
    "    plot_l_edge, plot_r_edge = l_edge, r_edge\n",
    "\n",
    "    if plot_r_edge - plot_l_edge < 2000:\n",
    "        increasing_d = 2000  - (plot_r_edge - plot_l_edge)\n",
    "        plot_l_edge -= increasing_d\n",
    "        plot_r_edge += increasing_d\n",
    "\n",
    "    fig, axs = plt.subplots(nrows=3, gridspec_kw={'hspace': 0.1})  # , sharex=True\n",
    "    \n",
    "    fig.set_figwidth(16)\n",
    "    fig.set_figheight(10)\n",
    "    \n",
    "    axs[0].set_title(f\"#{F_ID}\")\n",
    "    \n",
    "    b, a = signal.butter(5, 0.1)\n",
    "    d_alpha_d2f = signal.filtfilt(b, a, np.diff(d_alpha_f))\n",
    "    \n",
    "    axs[0].plot(range(plot_l_edge, plot_r_edge), d_alpha[plot_l_edge:plot_r_edge], label=\"D-alpha\", alpha=0.8, zorder=2)\n",
    "    # axs[0].plot(range(l_edge, r_edge), d_alpha_d1[l_edge:r_edge], label=\"Diff 1\")\n",
    "    axs[0].plot(range(plot_l_edge, plot_r_edge), d_alpha_f[plot_l_edge:plot_r_edge] * 10, label=\"Filtered D1 (x10)\", alpha=0.8)\n",
    "    axs[0].plot(range(plot_l_edge, plot_r_edge), d_alpha_d2f[plot_l_edge:plot_r_edge] * 100, label=\"D2 (x100)\", alpha=0.8)\n",
    "    axs[0].scatter([l_edge, r_edge], [0, 0], s=1000, color=\"black\", marker=\"|\", zorder=1)  # d_alpha[l_edge], d_alpha[r_edge]\n",
    "\n",
    "    start_time = time.time()\n",
    "    coef=1.\n",
    "    x = get_d1_crosses(d_alpha_f, d_alpha_d2f, l_edge, r_edge, d1_coef=coef)\n",
    "    print(f\"\\n------\\n------\\n\\n{ind + 1}/{res_slices_edges.shape[0]} - Slice ({l_edge/1e3}, {r_edge/1e3}) ms - mark: {slices_marks[ind]} (1 - dELM, 2 - LCO, 3 - EHO) - {len(x)} peaks - {(time.time() - start_time)*1e3:.3f} ms\")\n",
    "    # axs[0].scatter(x, d_alpha[x], s=20, color=\"black\")\n",
    "\n",
    "    if len(x) > 1:\n",
    "        print(f\"Start prossecing peaks ...\", end=\" \")  # \n",
    "        start_time = time.time()\n",
    "        res_groups_peaks = get_groups_from_signal(d_alpha, d_alpha_f, d_alpha_d2f, l_edge, r_edge)\n",
    "        # print(\"- logg: \", res_groups_peaks)\n",
    "        print(f\"- Tooks: {(time.time() - start_time)*1e3:.3f} ms\")\n",
    "        for g_i in range(len(res_groups_peaks)):\n",
    "            points = res_groups_peaks[g_i]\n",
    "            c = colors[g_i % len(colors)]\n",
    "            axs[0].scatter(points, d_alpha[points] + g_i * 0.05, s=20, color=c, zorder=0)\n",
    "            \n",
    "            m_d = get_time_delta(points) / 1e3\n",
    "            std_d = 0\n",
    "            for p_i in range(1, len(points)):\n",
    "                std_d += (m_d - (points[p_i] - points[p_i - 1]) / 1e3) ** 2\n",
    "            std_d = (std_d / len(points) / (len(points) - 1)) ** .5\n",
    "            print(f\"{g_i + 1}/{len(res_groups_peaks)} Group of peaks ({c}) - {len(points)} peaks in group - mean delta: {m_d:.3f} ms - freq: {1/m_d:.3f} +- {std_d/(m_d ** 2):.3f} kHz\")\n",
    "        \n",
    "        for p_i in range(len(x)):\n",
    "            num = p_i\n",
    "            d = 0\n",
    "            while num > 10:\n",
    "                num = num // 10\n",
    "                d += 1\n",
    "            axs[0].annotate(p_i, (x[p_i] - (25) * (r_edge - l_edge) / 5000 - 15 * d, d_alpha[x[p_i]] + 0.03))\n",
    "    \n",
    "    axs[0].axhline(d_alpha_f.mean(), color=\"red\", linestyle=':', linewidth=0.8)\n",
    "    axs[0].axhline((d_alpha_f.mean() + d_alpha_f.std()) * 10 * coef, color=\"green\", linestyle=':', linewidth=0.8)\n",
    "    axs[0].axhline((d_alpha_f.mean() - d_alpha_f.std()) * 10 * coef, color=\"green\", linestyle=':', linewidth=0.8)\n",
    "    axs[0].axhline((d_alpha_d2f.mean() + d_alpha_d2f.std()) * 100, color=\"blue\", linestyle=':', linewidth=0.8)\n",
    "    axs[0].axhline((d_alpha_d2f.mean() - d_alpha_d2f.std()) * 100, color=\"blue\", linestyle=':', linewidth=0.8)\n",
    "    axs[0].grid(which='major', color='#DDDDDD', linewidth=0.9)\n",
    "    axs[0].grid(which='minor', color='#DDDDDD', linestyle=':', linewidth=0.7)\n",
    "    axs[0].minorticks_on()\n",
    "    axs[0].xaxis.set_minor_locator(AutoMinorLocator(10))\n",
    "    axs[0].legend(loc='lower right')\n",
    "\n",
    "    # axs[1].plot(range(plot_l_edge, plot_r_edge), sxr[plot_l_edge:plot_r_edge], label=\"SXR\")\n",
    "    # # axs[1].plot(range(l_edge, r_edge), sxr_d1[l_edge:r_edge], label=\"Diff 1\")\n",
    "    # axs[1].plot(range(plot_l_edge, plot_r_edge), sxr_f[plot_l_edge:plot_r_edge] * 10, label=\"Filtered D1 (x10)\")\n",
    "\n",
    "    for i in channels[::2]:  # channels[::2]\n",
    "        axs[1].plot(range(plot_l_edge, plot_r_edge), dbs_df[f\"ch{i}_A\"][plot_l_edge:plot_r_edge], label=f\"DBS ch{i}_A\")\n",
    "        axs[2].plot(range(plot_l_edge, plot_r_edge), dbs_df[f\"ch{i}_dfi\"][plot_l_edge:plot_r_edge], label=f\"DBS ch{i}_dFi\")\n",
    "\n",
    "    axs[1].grid(which='major', color='#DDDDDD', linewidth=0.9)\n",
    "    axs[1].grid(which='minor', color='#DDDDDD', linestyle=':', linewidth=0.7)\n",
    "    axs[1].minorticks_on()\n",
    "    axs[1].xaxis.set_minor_locator(AutoMinorLocator(10))\n",
    "    axs[1].legend()\n",
    "\n",
    "    axs[2].grid(which='major', color='#DDDDDD', linewidth=0.9)\n",
    "    axs[2].grid(which='minor', color='#DDDDDD', linestyle=':', linewidth=0.7)\n",
    "    axs[2].minorticks_on()\n",
    "    axs[2].xaxis.set_minor_locator(AutoMinorLocator(10))\n",
    "    axs[2].legend()\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    mode = input(\"Input mode [manual (input points) - 0 | auto (input group) - 1 | continue - -1]: \")\n",
    "    while mode != \"\" and int(mode) >= 0:\n",
    "        mode = int(mode)\n",
    "        if mode == 0:\n",
    "            points_ind = list(map(int, input(\"Input point indexes:\\n\").strip().split()))\n",
    "            points = x[points_ind]\n",
    "        else:\n",
    "            gr_ind = int(input(f\"Input group number (from 1 to {len(res_groups_peaks)}): \").strip().split()[0]) - 1\n",
    "            points = res_groups_peaks[gr_ind]\n",
    "        \n",
    "        m_d = get_time_delta(points) / 1e3\n",
    "        std_d = 0\n",
    "        for p_i in range(1, len(points)):\n",
    "            std_d += (m_d - (points[p_i] - points[p_i - 1]) / 1e3) ** 2\n",
    "        std_d = (std_d / len(points) / (len(points) - 1)) ** .5\n",
    "        df_arr.append([points[0] - 50, points[-1] + 50, len(points), 1/m_d, std_d/(m_d ** 2), slices_marks[ind]])\n",
    "        # cur_deltas_desync_elm.std() / (cur_deltas_desync_elm.mean() ** 2)\n",
    "        mode = input(\"Input mode [manual (input points) - 0 | auto (input group) - 1 | continue - -1]: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>l_edge</th>\n",
       "      <th>r_edge</th>\n",
       "      <th>n</th>\n",
       "      <th>fr</th>\n",
       "      <th>fr_std</th>\n",
       "      <th>mark</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>145388</td>\n",
       "      <td>146203</td>\n",
       "      <td>9</td>\n",
       "      <td>11.188811</td>\n",
       "      <td>0.821436</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>146291</td>\n",
       "      <td>147160</td>\n",
       "      <td>10</td>\n",
       "      <td>11.703511</td>\n",
       "      <td>0.382058</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>147260</td>\n",
       "      <td>147579</td>\n",
       "      <td>4</td>\n",
       "      <td>13.698630</td>\n",
       "      <td>1.659071</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>147665</td>\n",
       "      <td>148379</td>\n",
       "      <td>8</td>\n",
       "      <td>11.400651</td>\n",
       "      <td>1.197985</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>148440</td>\n",
       "      <td>150033</td>\n",
       "      <td>20</td>\n",
       "      <td>12.726055</td>\n",
       "      <td>0.906851</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>150177</td>\n",
       "      <td>151545</td>\n",
       "      <td>13</td>\n",
       "      <td>9.463722</td>\n",
       "      <td>1.071310</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>151552</td>\n",
       "      <td>152673</td>\n",
       "      <td>5</td>\n",
       "      <td>3.917728</td>\n",
       "      <td>0.167389</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>153940</td>\n",
       "      <td>155158</td>\n",
       "      <td>6</td>\n",
       "      <td>4.472272</td>\n",
       "      <td>0.249527</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>155126</td>\n",
       "      <td>156619</td>\n",
       "      <td>17</td>\n",
       "      <td>11.486001</td>\n",
       "      <td>0.523387</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>156712</td>\n",
       "      <td>157220</td>\n",
       "      <td>5</td>\n",
       "      <td>9.803922</td>\n",
       "      <td>1.894502</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   l_edge  r_edge   n         fr    fr_std  mark\n",
       "0  145388  146203   9  11.188811  0.821436   0.0\n",
       "1  146291  147160  10  11.703511  0.382058   0.0\n",
       "2  147260  147579   4  13.698630  1.659071   0.0\n",
       "3  147665  148379   8  11.400651  1.197985   0.0\n",
       "4  148440  150033  20  12.726055  0.906851   0.0\n",
       "5  150177  151545  13   9.463722  1.071310   0.0\n",
       "6  151552  152673   5   3.917728  0.167389   0.0\n",
       "7  153940  155158   6   4.472272  0.249527   0.0\n",
       "8  155126  156619  17  11.486001  0.523387   0.0\n",
       "9  156712  157220   5   9.803922  1.894502   0.0"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# l_edge, r_edge, n, fr, fr std\n",
    "df = pd.DataFrame(sorted(df_arr, key=lambda x: x[0]), columns=[\"l_edge\", \"r_edge\", \"n\", \"fr\", \"fr_std\", \"mark\"])\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f\"data/df/stats/{F_ID}_slices_stats.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slices_df = pd.read_csv(proj_path + f\"/data/df/marks/{F_ID}_marks.csv\")\n",
    "slices_edges = slices_df.to_numpy()[:, :2]\n",
    "slices_marks = slices_df.to_numpy()[:, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 4  # 3, 4, 7\n",
    "l_edge, r_edge = slices_edges[ind]\n",
    "\n",
    "fig, ax1 = plt.subplots(nrows=1, sharex=True, gridspec_kw={'hspace': 0.2})\n",
    "\n",
    "fig.set_figwidth(16)\n",
    "fig.set_figheight(4)\n",
    "\n",
    "ax1.set_title(f\"#{F_ID}\")\n",
    "\n",
    "b, a = signal.butter(5, 0.1)\n",
    "d_alpha_d2f = signal.filtfilt(b, a, np.diff(d_alpha_f))\n",
    "\n",
    "ax1.plot(range(l_edge, r_edge), d_alpha[l_edge:r_edge], label=\"D-alpha\", alpha=0.8, zorder=2)\n",
    "# ax1.plot(range(l_edge, r_edge), d_alpha_d1[l_edge:r_edge], label=\"Diff 1\")\n",
    "ax1.plot(range(l_edge, r_edge), d_alpha_f[l_edge:r_edge] * 10, label=\"Filtered D1 (x10)\", alpha=0.8)\n",
    "ax1.plot(range(l_edge, r_edge), d_alpha_d2f[l_edge:r_edge] * 100, label=\"D2 (x100)\", alpha=0.8)\n",
    "\n",
    "coef=1.\n",
    "x = get_d1_crosses(d_alpha_f, d_alpha_d2f, l_edge, r_edge, d1_coef=coef)\n",
    "print(f\"{ind + 1}/{slices_edges.shape[0]} - Slice ({l_edge/1e3}, {r_edge/1e3}) ms - {len(x)} peaks - {(time.time() - start_time)*1e3:.3f} ms\")\n",
    "# ax1.scatter(x, d_alpha[x], s=20, color=\"black\")\n",
    "\n",
    "print(f\"Start prossecing peaks ...\", end=\" \")  # \n",
    "start_time = time.time()\n",
    "res_groups_peaks = get_groups_from_signal(d_alpha, d_alpha_f, d_alpha_d2f, l_edge, r_edge)\n",
    "# print(\"- logg: \", res_groups_peaks)\n",
    "print(f\"- Tooks: {(time.time() - start_time)*1e3:.3f} ms\")\n",
    "for g_i in range(len(res_groups_peaks)):\n",
    "    points = res_groups_peaks[g_i]\n",
    "    c = colors[g_i]\n",
    "    ax1.scatter(points, d_alpha[points] + g_i * 0.1, s=20, color=c, zorder=0)\n",
    "    m_d = get_time_delta(points) / 1e3\n",
    "    print(f\"{g_i + 1}/{len(res_groups_peaks)} Group of peaks ({c}) - {len(points)} peaks in group - mean delta: {m_d:.3f} ms - freq: {1/m_d:.3f} kHz\")\n",
    "\n",
    "for i in range(len(x)):\n",
    "    num = i\n",
    "    d = 0\n",
    "    while num > 10:\n",
    "        num = num // 10\n",
    "        d += 1\n",
    "    ax1.annotate(i, (x[i] - 25 - 15 * d, d_alpha[x[i]] + 0.03))\n",
    "\n",
    "ax1.axhline(d_alpha_f.mean(), color=\"red\", linestyle=':', linewidth=0.8)\n",
    "ax1.axhline((d_alpha_f.mean() + d_alpha_f.std()) * 10 * coef, color=\"green\", linestyle=':', linewidth=0.8)\n",
    "ax1.axhline((d_alpha_f.mean() - d_alpha_f.std()) * 10 * coef, color=\"green\", linestyle=':', linewidth=0.8)\n",
    "ax1.axhline((d_alpha_d2f.mean() + d_alpha_d2f.std()) * 100, color=\"blue\", linestyle=':', linewidth=0.8)\n",
    "ax1.axhline((d_alpha_d2f.mean() - d_alpha_d2f.std()) * 100, color=\"blue\", linestyle=':', linewidth=0.8)\n",
    "ax1.grid(which='major', color='#DDDDDD', linewidth=0.9)\n",
    "ax1.grid(which='minor', color='#DDDDDD', linestyle=':', linewidth=0.7)\n",
    "ax1.minorticks_on()\n",
    "ax1.xaxis.set_minor_locator(AutoMinorLocator(10))\n",
    "ax1.legend(loc='lower right')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# print(\"----\")\n",
    "# for j in range(len(slices_edges)):\n",
    "#     print(f\"{j + 1}/{slices_edges.shape[0]}\")\n",
    "#     l_edge, r_edge = slices_edges[j]\n",
    "#     start_time = time.time()\n",
    "#     res_groups = get_groups_from_signal(d_alpha, d_alpha_f, d_alpha_d2f, l_edge, r_edge)\n",
    "#     print(f\"Took: {(time.time() - start_time)*1e3:.3f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gr_delts = []\n",
    "for d in sorted(res_groups.keys()):  # , key=lambda k: res_groups[k][0]\n",
    "    delts = [d]\n",
    "    for d2 in res_groups.keys():\n",
    "        if abs(d - d2) / d <= 0.1 and d != d2 and len(res_groups[d2][0][1]) > 2 and d2 < 1000:\n",
    "            delts.append(d2)\n",
    "    if sorted(delts) not in gr_delts:\n",
    "        gr_delts.append(sorted(delts))\n",
    "        if len(res_groups[d][0][1]) > 2 and d < 1000:\n",
    "            for x in sorted(delts, key=lambda k: len(res_groups[k][0][1]), reverse=True):\n",
    "                print(f\"{x}: {res_groups[x]}\")\n",
    "            print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in sorted(res_groups.keys(), key=lambda k: res_groups[k][0]):\n",
    "    if res_groups[d][1][:2] == [0, 3]:\n",
    "        print(f\"{d}: {res_groups[d]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo for combine groups\n",
    "groups = [[0, 1, 2, 3, 4, 5, 6], [4, 5, 6, 8, 11], [8, 11, 14]]\n",
    "\n",
    "start_time = time.time()\n",
    "k = check_subsequention(groups[0], groups[1])\n",
    "print(f\"Took: {(time.time() - start_time)*1e6:.3e} mcs\")\n",
    "print(f\"Arrays: {groups[0]}, {groups[1]} - k = {k} - Res array: {groups[0] + groups[1][k:]}\")\n",
    "\n",
    "print(\"----\")\n",
    "\n",
    "start_time = time.time()\n",
    "res_grops = combine_groups(groups)\n",
    "print(f\"Took: {(time.time() - start_time)*1e6:.3f} mcs\")\n",
    "print(res_grops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non phisical\n",
    "def get_groups_naive(arr): # -> groups = {Delta: [mistake: float, group: list]}\n",
    "    n = len(arr)\n",
    "    res = {}\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            delta = arr[j] - arr[i]\n",
    "            group = [i, j]\n",
    "            err = 0.0\n",
    "            for k in range(j + 2, n):\n",
    "                if abs(arr[k] - (arr[group[-1]] + delta)) >= abs(arr[k - 1] - (arr[group[-1]] + delta)):\n",
    "                    cur_err = abs(arr[k - 1] - (arr[group[-1]] + delta)) - int(random.random() * 10)\n",
    "                    if err + cur_err > delta / 3 * 2:  #  / len(group)\n",
    "                        break\n",
    "                    err += cur_err\n",
    "                    group.append(k - 1)\n",
    "            if delta in res.keys():\n",
    "                res[delta].append([err, group])\n",
    "                res[delta].sort(key=lambda x: len(x[1]), reverse=True)\n",
    "            else:\n",
    "                res[delta] = [[err, group]]\n",
    "    return res\n",
    "\n",
    "# def get_groups_dp(): # -> groups = {Delta: [mistake: float, group: list]}\n",
    "# def get_groups_gen(): # -> groups = {Delta: [mistake: float, group: list]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T14:59:44.820246Z",
     "start_time": "2024-09-09T14:59:43.934085Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import AutoMinorLocator\n",
    "\n",
    "\n",
    "mode_plotting = 0  # int(input(\"Input visualising mode [ 0 - manual | 1 - only marked ]:\"))\n",
    "# mode_marking = int(input(\"Input marking mode (0 - manual | 1 - semiauto): \").strip().split()[0])\n",
    "l_edge = 184000  # int(input(f\"Input start index [0:{df.shape[0]//1000*1000}]: \").strip().split()[0])\n",
    "# print(\"---\")\n",
    "step = 2000\n",
    "width = 5000\n",
    "\n",
    "step_away = 10\n",
    "\n",
    "while l_edge < df.shape[0]:\n",
    "    r_edge = min(l_edge + width, df.shape[0] - 1)\n",
    "    \n",
    "    if mode_plotting == 1 and sum(mark_d_alpha[l_edge:r_edge]) == 0 and sum(mark_sxr[l_edge:r_edge]) == 0:\n",
    "        l_edge += step\n",
    "        continue\n",
    "    \n",
    "    fig, [ax1, ax2, ax3] = plt.subplots(nrows=3, sharex=True, gridspec_kw={'hspace': 0.2})\n",
    "\n",
    "    fig.set_figwidth(16)\n",
    "    fig.set_figheight(12)\n",
    "    \n",
    "    ax1.set_title(f\"#{F_ID}\")\n",
    "\n",
    "    b, a = signal.butter(5, 0.1)\n",
    "    d_alpha_d2f = signal.filtfilt(b, a, np.diff(d_alpha_f))\n",
    "\n",
    "    ax1.plot(range(l_edge, r_edge), d_alpha[l_edge:r_edge], label=\"D-alpha\", alpha=0.8)\n",
    "    # ax1.plot(range(l_edge, r_edge), d_alpha_d1[l_edge:r_edge], label=\"Diff 1\")\n",
    "    ax1.plot(range(l_edge, r_edge), d_alpha_f[l_edge:r_edge] * 10, label=\"Filtered D1 (x10)\", alpha=0.8)\n",
    "    ax1.plot(range(l_edge, r_edge), d_alpha_d2f[l_edge:r_edge] * 100, label=\"D2 (x100)\", alpha=0.8)\n",
    "\n",
    "    x = get_d1_crosses(d_alpha_f, d_alpha_d2f, l_edge, r_edge)\n",
    "    print(get_groups_from_signal(d_alpha_f, d_alpha_d2f, l_edge, r_edge))\n",
    "    ax1.scatter(x, d_alpha[x], s=20, color=\"black\")\n",
    "\n",
    "    # ax1.plot(range(l_edge, r_edge), mark_d_alpha_2[l_edge:r_edge] * 0.4, label=\"Mark (x0.4)\", color=\"red\")\n",
    "    # ax1.plot(range(l_edge, r_edge), mark_d_alpha[l_edge:r_edge] * 0.5, label=\"Mark proc (x0.5)\", color=\"black\")\n",
    "    ax1.axhline(0, color=\"red\", linestyle=':', linewidth=0.8)\n",
    "    ax1.axhline((d_alpha_f.std()) * 10, color=\"green\", linestyle=':', linewidth=0.8)\n",
    "    ax1.axhline((- d_alpha_f.std()) * 10, color=\"green\", linestyle=':', linewidth=0.8)\n",
    "    ax1.axhline((np.diff(d_alpha_f).std()) * 100, color=\"blue\", linestyle=':', linewidth=0.8)\n",
    "    ax1.axhline((- np.diff(d_alpha_f).std()) * 100, color=\"blue\", linestyle=':', linewidth=0.8)\n",
    "    # ax1.axhline((meta_da.d_q + meta_da.d_std * meta_da.d_std_bottom) * 10, color=\"green\", linestyle=':', linewidth=0.8)\n",
    "    # ax1.axhline((meta_da.d_q - meta_da.d_std * meta_da.d_std_bottom) * 10, color=\"green\", linestyle=':', linewidth=0.8)\n",
    "    # ax1.axhline((meta_da.d_q + meta_da.d_std * meta_da.d_std_top) * 10, color=\"blue\", linestyle=':', linewidth=0.8)\n",
    "    # ax1.axhline((meta_da.d_q - meta_da.d_std * meta_da.d_std_top) * 10, color=\"blue\", linestyle=':', linewidth=0.8)\n",
    "    ax1.grid(which='major', color='#DDDDDD', linewidth=0.9)\n",
    "    ax1.grid(which='minor', color='#DDDDDD', linestyle=':', linewidth=0.7)\n",
    "    ax1.minorticks_on()\n",
    "    ax1.xaxis.set_minor_locator(AutoMinorLocator(10))\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.plot(range(l_edge, r_edge), sxr[l_edge:r_edge], label=\"SXR\")\n",
    "    # ax2.plot(range(l_edge, r_edge), sxr_d1[l_edge:r_edge], label=\"Diff 1\")\n",
    "    ax2.plot(range(l_edge, r_edge), sxr_f[l_edge:r_edge] * 10, label=\"Filtered D1 (x10)\")\n",
    "    ax2.plot(range(l_edge, r_edge), mark_sxr[l_edge:r_edge] * 0.5, label=\"Mark proc (x0.5)\", color=\"black\")\n",
    "    ax2.axhline(meta_sxr.d_q * 10, color=\"red\", linestyle=':', linewidth=0.7)\n",
    "    ax2.axhline((meta_sxr.d_q + meta_sxr.d_std * meta_sxr.d_std_bottom) * 10, color=\"green\", linestyle=':', linewidth=0.7)\n",
    "    ax2.axhline((meta_sxr.d_q - meta_sxr.d_std * meta_sxr.d_std_bottom) * 10, color=\"green\", linestyle=':', linewidth=0.7)\n",
    "    ax2.axhline((meta_sxr.d_q + meta_sxr.d_std * meta_sxr.d_std_top) * 10, color=\"blue\", linestyle=':', linewidth=0.7)\n",
    "    ax2.axhline((meta_sxr.d_q - meta_sxr.d_std * meta_sxr.d_std_top) * 10, color=\"blue\", linestyle=':', linewidth=0.7)\n",
    "\n",
    "    ax2.grid(which='major', color='#DDDDDD', linewidth=0.9)\n",
    "    ax2.grid(which='minor', color='#DDDDDD', linestyle=':', linewidth=0.7)\n",
    "    ax2.minorticks_on()\n",
    "    ax2.xaxis.set_minor_locator(AutoMinorLocator(10))\n",
    "    ax2.legend()\n",
    "\n",
    "    # mgd_filtered = np.copy(mgd)\n",
    "    # mgd_filtered[abs(mgd_filtered - mgd.mean()) > 5 * mgd.std()] = mgd.mean()\n",
    "    \n",
    "    b, a = signal.butter(5, 0.2)\n",
    "    mgd_f = signal.filtfilt(b, a, mgd)\n",
    "\n",
    "    # mgd_d1 = np.diff(mgd_f)\n",
    "    # mgd_ff = signal.filtfilt(b, a, mgd_d1)\n",
    "\n",
    "    mgd_f_filtered = np.copy(mgd_f)\n",
    "    mgd_f_filtered[abs(mgd_f_filtered - mgd_f.mean()) > 5 * mgd_f.std()] = mgd_f.mean()\n",
    "\n",
    "    mgd_f_d1 = np.diff(mgd_f)\n",
    "    \n",
    "    ax3.plot(range(l_edge, r_edge), mgd[l_edge:r_edge], label=\"MGD abs\")  # semilogy np.diff(sxr)\n",
    "    # ax3.plot(range(l_edge, r_edge), mgd_f_d1[l_edge:r_edge], label=\"Filtered D1 (x10)\")  # semilogy np.diff(sxr)\n",
    "    # ax3.plot(range(l_edge, r_edge), mgd_f_filtered[l_edge:r_edge], label=\"Filtered MGD\")  # semilogy np.diff(sxr)\n",
    "    ax3.axhline(mgd_f.mean(), color=\"red\", linestyle=':', linewidth=0.7)\n",
    "    # ax3.axhline(mgd_ff.mean(), color=\"red\", linestyle=':', linewidth=0.7)\n",
    "    # ax3.axhline((mgd_ff.mean() + 1 * mgd_f_filtered.std()) * 1, color=\"green\", linestyle=':', linewidth=0.7)\n",
    "    # ax3.axhline((mgd_ff.mean() - 1 * mgd_f_filtered.std()) * 1, color=\"green\", linestyle=':', linewidth=0.7)\n",
    "    ax3.axhline((mgd_f.mean() + 1 * mgd.std()) * 1, color=\"blue\", linestyle=':', linewidth=0.7)\n",
    "    ax3.axhline((mgd_f.mean() - 1 * mgd.std()) * 1, color=\"blue\", linestyle=':', linewidth=0.7)\n",
    "    ax3.legend()\n",
    "\n",
    "    ax3.grid(which='major', color='#DDDDDD', linewidth=0.9)\n",
    "    ax3.grid(which='minor', color='#DDDDDD', linestyle=':', linewidth=0.7)\n",
    "    ax3.minorticks_on()\n",
    "    ax3.xaxis.set_minor_locator(AutoMinorLocator(10))\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    # res = list(map(int, input(f\"Input index pairs of ELM fragments ({df.t[l_edge]} - {df.t[r_edge]} ms):\\n\").strip().split()))\n",
    "    # for i in range(0, len(res), 2):\n",
    "    #     board_ind = [res[i], res[i+1]]\n",
    "\n",
    "    #     mark = float(input(\"Input mark to set [ 0 | 1 | 2 ]: \"))\n",
    "        \n",
    "    #     if mode_marking and mark > 0:\n",
    "    #         board_ind = get_borders(df.loc[res[i]:res[i + 1], \"ch1\"].to_numpy(), scale=1.5)\n",
    "    #         board_ind[0] = max(board_ind[0] - step_away + res[i], 0)\n",
    "    #         board_ind[1] += res[i]\n",
    "        \n",
    "    #     df.loc[board_ind[0]:board_ind[1], \"ch1_marked\"] = mark\n",
    "\n",
    "    print(np.argwhere(abs(mgd[l_edge: r_edge] - mgd.mean()) >  mgd.std()) + l_edge)\n",
    "\n",
    "    print(f\"ELM fragment ({df.t[l_edge]} - {df.t[r_edge]} ms)\\n\")\n",
    "    break\n",
    "    # input(f\"ELM fragment ({df.t[l_edge]} - {df.t[r_edge]} ms)\\n\")\n",
    "\n",
    "    plt.close()\n",
    "    plt.clf()\n",
    "    l_edge += step\n",
    "\n",
    "# 217500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T14:35:16.347379Z",
     "start_time": "2024-09-09T14:35:16.344759Z"
    }
   },
   "source": [
    "---\n",
    "### Reporting sync ELM program\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filename = f\"{F_ID}\"\n",
    "\n",
    "sxr_slices = [Slice(0, 0)] + get_slices(mark_sxr)\n",
    "deltas_sync_elm = np.full(shape=len(sxr_slices), fill_value=np.nan)\n",
    "count_desync_elm = np.full(shape=len(sxr_slices), fill_value=np.nan)\n",
    "deltas_desync_elm = np.full(shape=len(sxr_slices), fill_value=np.nan)\n",
    "amplitudes_sync_mgd = np.full(shape=len(sxr_slices), fill_value=np.nan)\n",
    "deltas_sync_mgd = np.full(shape=len(sxr_slices), fill_value=np.nan)\n",
    "amplitudes_desync_mgd = np.full(shape=len(sxr_slices), fill_value=np.nan)\n",
    "deltas_desync_mgd = np.full(shape=len(sxr_slices), fill_value=np.nan)\n",
    "\n",
    "report_lines = []\n",
    "\n",
    "report_lines.append(\"\\n----------------\\n\")\n",
    "report_lines.append(f\"Signal {filename}\\n\")\n",
    "report_lines.append(f\"SXR falls: {len(sxr_slices) - 1}\\n\")\n",
    "report_lines.append(\"-----\\n\")\n",
    "l_shift, r_shift = 100, 1500\n",
    "# logg 6\n",
    "print(\"-\", end=\"\")\n",
    "\n",
    "for sl_i in range(1, len(sxr_slices)):\n",
    "    sxr_pointer = np.argmin(sxr_f[sxr_slices[sl_i].l: sxr_slices[sl_i].r]) + sxr_slices[sl_i].l\n",
    "    \n",
    "    da_slices = get_slices(mark_d_alpha[sxr_slices[sl_i - 1].r: min(sxr_slices[sl_i].r + r_shift, mark_d_alpha.shape[0])])\n",
    "    mgd_slice = sxr_slices[sl_i].copy()\n",
    "    mgd_slice.expand(l_shift)\n",
    "\n",
    "    deltas_sync_mgd[sl_i - 1] = (np.argmax(mgd[mgd_slice.l: mgd_slice.r]) + mgd_slice.l - sxr_pointer) / 1e3\n",
    "    amplitudes_sync_mgd[sl_i - 1] = abs(mgd[mgd_slice.l: mgd_slice.r].max() - mgd.mean())\n",
    "    \n",
    "    if deltas_sync_mgd[sl_i - 1] >= 0.5:\n",
    "        deltas_sync_mgd[sl_i - 1] = np.nan\n",
    "        sync_mgd_info = f\"-- No nearest peaks on MGD\"\n",
    "    else:\n",
    "        sync_mgd_info = f\"-- MGD peak: delta = {deltas_sync_mgd[sl_i - 1]:3.3f} ms, amplitude = {amplitudes_sync_mgd[sl_i - 1]:3.3f}\"\n",
    "\n",
    "    if len(da_slices) == 0:\n",
    "        deltas_sync_elm[sl_i - 1] = np.nan\n",
    "        sync_elm_info = f\"-- No sync ELM on D-alpha\"\n",
    "        desync_elm_info = f\"-- No desync ELM on D-alpha\"  # + \" \" * 15\n",
    "    else:\n",
    "        ind = 0\n",
    "        while sxr_slices[sl_i].r - (da_slices[ind].r + sxr_slices[sl_i - 1].l - l_shift) > 0 and ind + 1 < len(da_slices):\n",
    "            ind += 1\n",
    "\n",
    "        sync_elm_slice = da_slices[ind].copy()\n",
    "        sync_elm_slice.move(sxr_slices[sl_i - 1].l)\n",
    "        deltas_sync_elm[sl_i - 1] = (np.argmax(d_alpha_f[sync_elm_slice.l: sync_elm_slice.r]) + sync_elm_slice.l - sxr_pointer) / 1e3\n",
    "        \n",
    "        if deltas_sync_elm[sl_i - 1] >= 0.5:  #  or deltas_sync_elm[sl_i] < - l_shift / 1e3\n",
    "            deltas_sync_elm[sl_i - 1] = np.nan\n",
    "            sync_elm_info = f\"-- No sync ELM on D-alpha\"\n",
    "        else:\n",
    "            sync_elm_info = f\"-- Sync ELM: d = {deltas_sync_elm[sl_i - 1]:3.3f} ms\"\n",
    "\n",
    "        if ind - 1 <= 0:\n",
    "            count_desync_elm[sl_i - 1] = np.nan\n",
    "            desync_elm_info = f\"-- No desync ELM on D-alpha\"  # + \" \" * 15\n",
    "            desync_mgd_info = \"\"\n",
    "        else:\n",
    "            count_desync_elm[sl_i - 1] = ind - 1\n",
    "            \n",
    "            cur_deltas_desync_elm = np.zeros(int(count_desync_elm[sl_i - 1] - 1))\n",
    "            cur_desync_mgd_amplitude = np.zeros(int(count_desync_elm[sl_i - 1]))\n",
    "            cur_desync_mgd_deltas = np.zeros(int(count_desync_elm[sl_i - 1]))\n",
    "\n",
    "            prev_argmax = np.argmax(d_alpha_f[da_slices[1].l: da_slices[1].r]) + da_slices[1].l\n",
    "            \n",
    "            cur_desync_mgd_amplitude[0] = abs(mgd[da_slices[1].l - 10: da_slices[1].r + 10].max() - mgd.mean())\n",
    "            cur_desync_mgd_deltas[0] = (np.argmax(mgd[da_slices[1].l - 10: da_slices[1].r + 10]) + da_slices[1].l - 10 - prev_argmax) / 1e3\n",
    "            \n",
    "            for elm_ind in range(1, ind - 1):\n",
    "                cur_argmax = np.argmax(d_alpha_f[da_slices[elm_ind].l: da_slices[elm_ind].r]) + da_slices[elm_ind].l\n",
    "                cur_deltas_desync_elm[elm_ind - 1] = (cur_argmax - prev_argmax) / 1e3\n",
    "                \n",
    "                cur_desync_mgd_amplitude[elm_ind] = abs(mgd[da_slices[elm_ind].l - 100: da_slices[elm_ind].r + 100].max() - mgd.mean())\n",
    "                cur_desync_mgd_deltas[elm_ind] = (np.argmax(mgd[da_slices[elm_ind].l - 100: da_slices[elm_ind].r + 100]) + da_slices[elm_ind].l - 100 - cur_argmax) / 1e3\n",
    "                \n",
    "                prev_argmax = cur_argmax\n",
    "\n",
    "            deltas_desync_elm[sl_i - 1] = cur_deltas_desync_elm.mean() if count_desync_elm[sl_i - 1] - 1 > 0 else np.nan\n",
    "            amplitudes_desync_mgd[sl_i - 1] = np.nanmean(cur_desync_mgd_amplitude)\n",
    "            deltas_desync_mgd[sl_i - 1] = np.nanmean(cur_desync_mgd_deltas)\n",
    "            desync_elm_info = f\"-- Desync ELM: count = {count_desync_elm[sl_i - 1]}\"\n",
    "            if count_desync_elm[sl_i - 1] > 1 and cur_deltas_desync_elm.mean() > 1e-6:\n",
    "                desync_elm_info += f\", fr mean = {1 / cur_deltas_desync_elm.mean():3.3f} kGz, fr std = {cur_deltas_desync_elm.std() / (cur_deltas_desync_elm.mean() ** 2):3.3f} kGz\"  # d mean = {desync_elm_deltas.mean():3.3f}, d std = {desync_elm_deltas.std():3.3f} ms, \n",
    "            else:\n",
    "                desync_elm_info += f\", fr mean = nan kGz, fr std = nan kGz\"\n",
    "            desync_mgd_info = f\"-- MGD peaks: deltas mean = {cur_desync_mgd_deltas.mean():3.3f} ms, deltas std = {cur_desync_mgd_deltas.std():3.3f} ms, \"\n",
    "            desync_mgd_info += f\"A mean = {cur_desync_mgd_amplitude.mean():6.6f}, A std = {cur_desync_mgd_amplitude.std():6.6f}\"\n",
    "        \n",
    "    \n",
    "    report_lines.append(f\"{sl_i}. SXR fall - {sxr_pointer / 1e3:3.3f} ms {sync_elm_info}\\n\\t{sync_mgd_info}\\n\\t{desync_elm_info}\\n\\t{desync_mgd_info}\\n\")\n",
    "# logg 8\n",
    "print(\"-\", end=\"\")\n",
    "\n",
    "\n",
    "report_lines.append(\"-----\\n\")\n",
    "report_lines.append(f\"Sync ELM info: deltas mean = {np.nanmean(deltas_sync_elm):.3f} ms, deltas std = {np.nanstd(deltas_sync_elm):.3f} ms\\n\")\n",
    "report_lines.append(f\"Desync ELM info: count mean = {np.nanmean(count_desync_elm):.3f}, count std = {np.nanstd(count_desync_elm):.3f}, \" +\n",
    "                    f\"frequency mean = {1 / np.nanmean(deltas_desync_elm):.3f} ms, frequency std = {np.nanstd(deltas_desync_elm) / (np.nanmean(deltas_desync_elm) ** 2):.3f}\\n\")\n",
    "report_lines.append(f\"MGD peaks info (sync ELM): deltas mean = {np.nanmean(deltas_sync_mgd):.3f} ms, deltas std = {np.nanstd(deltas_sync_mgd):.3f} ms, \" +\n",
    "                    f\"A mean = {np.nanmean(amplitudes_sync_mgd):.3f}, A std = {np.nanstd(amplitudes_sync_mgd):.3f}\\n\")\n",
    "report_lines.append(f\"MGD peaks info (desync ELM): deltas mean = {np.nanmean(deltas_desync_mgd):.3f} ms, \" +\n",
    "                    f\"A mean = {np.nanmean(amplitudes_desync_mgd):.3f}\\n\")\n",
    "report_lines.append(\"-----\\n\")\n",
    "report_lines.append(f\"SXR falls w/o sync ELM in nearest area (-{l_shift * 1e-3} ms; {r_shift * 1e-3} ms): {np.count_nonzero(np.isnan(deltas_sync_elm))}\\n\")\n",
    "report_lines.append(f\"SXR falls w/o peaks on MGD in nearest area (-{l_shift * 1e-3} ms; {l_shift * 1e-3} ms): {np.count_nonzero(np.isnan(deltas_sync_mgd))}\\n\")\n",
    "report_lines.append(\"-----\\n\")\n",
    "report_lines.append(f\"SXR signal info: diff_quantile = {meta_sxr.d_q:.6f}, diff_std = {meta_sxr.d_std:.6f}\\n\")\n",
    "report_lines.append(f\"MGD signal info: mean = {mgd.mean():.6f}, std = {mgd.std():.6f}\\n\")\n",
    "report_lines.append(\"-----\\n\")\n",
    "report_lines.append(f\"SXR diff_std_top_edge: {meta_sxr.d_std_bottom:.3f} (approximate w/ a*exp^b)\\n\")\n",
    "report_lines.append(\"----------------\\n\")\n",
    "# logg 9\n",
    "print(\"-\", end=\"\")\n",
    "\n",
    "print(\"\".join(report_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T14:35:16.569514Z",
     "start_time": "2024-09-09T14:35:16.553867Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def process_fragments(data: np.array, mark_data: np.array, length_edge=10, distance_edge=25, scale=1.5, step_out=10) -> np.array:\n",
    "    proc_slice = Slice(0, 0)\n",
    "    cur_slice = Slice(0, 1)\n",
    "    f_fragment = False\n",
    "\n",
    "    while cur_slice.r < mark_data.shape[0]:\n",
    "        if mark_data[cur_slice.r] == 1.0:\n",
    "            if not f_fragment:\n",
    "                f_fragment = True\n",
    "        elif f_fragment:\n",
    "            # print(start_ind, end_ind)\n",
    "            if scale <= 1:\n",
    "                if not cur_slice.check_length(length_edge):\n",
    "                    mark_data[cur_slice.l: cur_slice.r] = 0.0\n",
    "                elif not proc_slice.collide_slices(cur_slice, distance_edge):\n",
    "                    mark_data[proc_slice.l: proc_slice.r] = 1.0\n",
    "                    proc_slice.copy(cur_slice)\n",
    "            elif scale:\n",
    "                mark_data[cur_slice.l: cur_slice.r] = 0.0\n",
    "                if cur_slice.check_length(length_edge):\n",
    "                    boarders = get_boarders(data[cur_slice.l: cur_slice.r], scale)\n",
    "                    # print(boards)\n",
    "                    boarders[0] = max(boarders[0] + cur_slice.l - step_out, 0)\n",
    "                    boarders[1] = min(boarders[1] + cur_slice.l, mark_data.shape[0])\n",
    "\n",
    "                    mark_data[boarders[0]:boarders[1]] = 1.0\n",
    "\n",
    "            f_fragment = False\n",
    "            cur_slice.collapse_borders()\n",
    "        elif not f_fragment:\n",
    "            cur_slice.collapse_borders()\n",
    "            if proc_slice.is_null():\n",
    "                proc_slice.copy(cur_slice)\n",
    "\n",
    "        cur_slice.step()\n",
    "\n",
    "    return mark_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
